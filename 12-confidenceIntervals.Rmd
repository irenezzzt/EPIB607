---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Parameter Intervals  {#CI}

The **objective** of this section is to provide simple examples of **reverse engineering** that show some of the logic behind statistical 'confidence intervals' for parameters. We begin with '100% confidence' intervals, and then, in the following subsections, we explain why we have to move to 'less-than-100% confidence' intervals, where things get a bit more nuanced.
In both sections, we emphasize the reverse engineering, i.e, by using as our limits the worst-case or almost-worst-case scenarios involving the (unknown) values of the parameter that is being estimated.  

## '100% confidence' intervals

### Example 1

Consider a very 'particularistic' parameter, the height of a particular  building. There is nothing 'scientific' about the parameter, except maybe that we use tools of mathematical science (of trignometry) to measure it. Nevertheless, we will sometimes refer to it one of the generic symbols for a parameter, namely $\theta.$

Suppose you measure the height of this building by standing a known horizontal distance (e.g. 100 metres) from the bottom of the building and using an instrument to measure the angle between the horizontal and the top of the building. Suppose, as shown in the left panel of Figure \@ref(fig:building) below, that the instrument gives a reading of 70 degrees. 

Remembering from trigonometry that the tangent of a 70 degree angles is 2.75, the angle of 70 degrees suggests that the height of the building  is $\hat{\theta}$ = 275 metres. The 'hat' is statistical shorthand for 'estimate of.' Since it is sometimes referred to as a 'point estimate' of $\theta$, we display the value using a dot or point. 

After calculating this, you learn that the measuring instrument only displays the angle is to the nearest 10 degrees. This means that the true angle is somewhere between 65 and 75 degrees.[^9_17] 

[^9_17]: This is the same  range you would get if it was dark and you used a laser pointer or flashlight attached to a wheel that rotates in fixed 10-degree steps, i.e., 5 degrees, 15 degrees, 25 degrees, etc.  At 65 degrees, the light is visible on the building, but at 75 degrees, it goes above the building and shines into the sky.

So you **cannot say** that the true height is **exactly** 275 metres. What **can** you say? And with what certainty?

You can put **limits** on the true height by asking **what are the minimum and maximum heights that could have produced the observed reading of 70 degrees?**

To do this you need to take the limits one at a time. The **minimum** angle that could have given the (rounded) readout of 70 degrees is 65 degrees, and this corresponds to a minimum height (lower limit) height of $\theta_L$  = 214 metres. The **maximum** angle that could have given the readout of 70 degrees is 75 degrees, and this corresponds to a maximum height (upper limit) of $\theta_U$ = 373 metres. Thus, assuming that the instrument is measuring the angle correctly, and then doing what you are told it does, you are 100% confident that the true height lies in the interval (214, 373). As is clear in the graph, this does not have the typical 275 $\pm$ a single-number (or in sybols, $\hat{\theta} \pm$ ME ['margin of error']) that we typically see in reports.

```{r building, eval=T, echo=F, fig.align="center", warning=FALSE, message=F,fig.cap="Estimating the height of an building by measuring subtended angles. The '70' in the left panel signifies that the real angle was somewhere between 65 and 75 degrees; thus the real height lies between the L and U limits of 214 and 373 metres. In the righ panel, the  interval shown by the thicker black segment to the right of the 3 individual intervals is the  set of parameter values common to all 3."}


DA = 10

xmax=0.75
DX=3+xmax
ymax=4

u = 100

par(mfrow=c(1,2),mar = c(0.01,1,0.01,0.25))

for (example in 1:2) {
	
 
plot(u*c(-3,xmax),u*c(-0.5,ymax),col="white",
 xaxt="n", yaxt="n",xlab="",ylab="",
 xlim=u*c(-3.35,xmax+0.35),ylim=u*c(-0.5,ymax),
 frame=FALSE)
lines(u*c(-4,0,0),u*c(0,0,ymax),lty="dotted")

segments(u*xmax, 0, u*xmax,  u*6)
for(h in seq(0,6,1)){
	text(u*xmax,u*h,
	     paste(toString(u*h),"m",sep=""),
	     adj=c(-0.2,0.5),cex=0.75)
	segments(u*(xmax-0.025), u*h, u*xmax,  u*h)
}
for(d in seq(1,3,1)){
	text(-u*d,-0.15*u,
	     paste(toString(u*d),"m",sep=""),
	     adj=c(0.5,1),cex=0.75)
	segments(-u*d, 0, -u*d,  -u*0.05)
}


H = u*2.25

x=0.15
U=u*5
L=0
COL=1
dd = 1

distances = 1
if(example==2) distances = seq(dd,3,dd)

for (d in u*distances ) {	
   ANGLE   = atan( H/d ) * 360/(2*pi) 
   angle.hat   = DA*round(ANGLE/DA)
   h.hat   = d*tan(  angle.hat       / ( 360/(2*pi) ) )
   segments(-d,0,0,h.hat,col=COL)
   ii = d/u

   text(-d + u*c(0.1,0.2,0.25)[ii], 0,
        paste("'",toString(angle.hat),"'",sep="") ,
        adj=c(0,-0.3), col=COL)
   h.upper = d*tan( (angle.hat+DA/2) / ( 360/(2*pi) ) )
   h.lower = d*tan( (angle.hat-DA/2) / ( 360/(2*pi) ) )
   segments(u*x,h.lower,u*x,h.upper,col=COL)
   if(example==1){
       text(u*x, h.upper, expression(theta[U]),adj=c(-0.5,0.5))
       text(u*x, h.lower, expression(theta[L]),adj=c(-0.5,0.5))
   } 
   points(u*x,h.hat,pch=19,cex=0.4,col=COL)
   if(example==1) points(u*xmax,h.hat,pch=19,cex=0.4,col=COL)
   U = min(U,h.upper)
   L = max(L,h.lower)
   x=x+0.15
   COL=COL+1
}
if(example==2){
  segments(u*(xmax),L,u*(xmax),U,lwd=4)
  segments(u*(xmax-0.075),L,u*(xmax+0.075),L )
  segments(u*(xmax-0.075),U,u*(xmax+0.075),U )
} 

}

```



***More data***

The panel on the right shows how, by obtaining 3 measurements at 3 different distances, and finding the interval they have in common (the overlap), you can narrow the interval within which the true height must lie.

**What allows us to be 100% confident in the parameter interval**

The reason is  the limited error range.  How wide the error range is, and how many measurements one makes, determine how wide the parameter interval is. 

### Example 2

This one is less artificial, and indeed is motivated by a real court case in the late 1990s in Quebec, where a defendant's age (which would determine whether he was tried in an adult or a juvenile court) was in doubt. He was adopted, while still a young child, from another country. Official birth records were not available, and his adoptive parents were able to get a cheaper airfare by claiming that he was under age 2 at the time. Bone age, and Tanner Staging, also known as Sexual Maturity Rating (SMR), an objective classification system used to track the development and sequence of secondary sex characteristics of children during puberty, were other pieces of information used by the judge.

For more on this topic of determining chronological age, see this article, entitled [Many applications for medical statistics](https://discovery.ucl.ac.uk/id/eprint/1470308/1/Tim_Cole_Intl_Innovation_140_Research_Media.pdf) and thos one, entitled [People smugglers, statistics and bone age](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2012.00568.x), by UCL statistics professor and child growth expert, [Tim Cole](https://scholar.google.com/citations?user=1P_yQocAAAAJ&hl=en).

Again, the person's correct chronological age is a  particularistic parameter, one that has nothing to do with science, or universal laws of Nature. But it can be estimated by using the laws of mathematics and statistics.

For didactic purposes, we will simplify matters, and assume that 'our' indirect method gives estimates such that if many of them were averaged, they would give the correct chronological age of the person (in statistical lingo, statisticians say that the method/estimator is 'unbiased'). However, as is seen below in Figure \ref@(fig:ciage), the individual measurements vary quite a bit around the correct age. They can be off by as much as 25% (1/4th) in either direction.[^9_2] Another unrealistic feature of our 'measurement model' is that the 'error distribution' has a **finite range**. The  **shape** of the error distribution doesn't come into the 100% 'confidence intervals' below, but it will matter a little bit -- but not a whole lot unless the sample size is small -- later on when we cut corners. 
  
[^9_2]: In practice, a measuring instrument  with this much measurement error would not be useful -- unless it was fast, safe, inexpensive, non-invasive, easily repeated, and so on -- but we make the measurement variations  this large just so we can see the patters more clearly on the graph!


```{r ciage,eval=T, echo=F, fig.align="center", warning=FALSE, message=F, fig.width=6, fig.height=5}

dSemiElliptical = function(q,mean=0,radius=1) ( 2/(pi*radius^2) ) *
                                           sqrt(radius^2 - (q-mean)^2)

pSemiElliptical =  function(q,mean=0,radius=1) 1/2 + 
          (q*sqrt(radius^2-q^2))/(pi*radius^2) + asin(q/radius)/pi 

qSemiElliptical = function(p,mean=0,radius=1){
  cdf = function(x) pSemiElliptical(x) - p 
  root = uniroot(cdf, c(-1,1))$root
  return(mean + root*radius)
}

rSemiElliptical =  function(n=1,mean=0,radius=1) {
   q = rep(NA,n)
   Max = dSemiElliptical(mean,mean,radius)
   i = 0
   while(i < n){
   	 u = mean + runif(1,-radius,radius)
     h = runif(1,0,Max)
     d = dSemiElliptical(u,mean,radius)
     if( h < d ) {i=i+1; q[i] = u}
   }
   return(q)
}

AGE = 18   # (7/8)*AGE

radius.as.proportion = 0.125

set.seed(1345437)
set.seed(2454375)
  
age.hats = rSemiElliptical(4, AGE, radius.as.proportion*AGE)

draw = function(conf.level){
	
  q.u = qSemiElliptical( (1+conf.level)/2 )
  
  half.alpha = toString( (1-conf.level)/2 )
  
  a.min=14; a.max=22; da=2
  beta = 1

  dx=75
  dy=0.4
  
  a.bottom = a.min - 1.75*da
  
  AGE = 18   # (7/8)*AGE
  
  par(mfrow=c(1,2),mar = c(0,0,0,0))
  
  
  for (example in 1:2) {
    
      n=1 ; if(example>1) n=4
      
      age.hat = age.hats[1:n]

      age.bar = mean(age.hat)
      
     div = 1; if (conf.level < 1) div=sqrt(n)
     u.at.a.max = (1+radius.as.proportion/div)*beta*(a.max+1)
     l.at.a.max = (1-radius.as.proportion/div)*beta*(a.max+1)
     
     u.at.a.min = (1+radius.as.proportion/div)*beta*a.min
     l.at.a.min = (1-radius.as.proportion/div)*beta*a.min
     
     plot(c(a.min,a.max),c(l.at.a.min,u.at.a.max),col="white",
          xlim=c(a.min-1.35*da/2,a.max+2.75*da/2),
          ylim=c(a.min-2.6*da,a.max+1.6*da), frame=FALSE,
          xaxt ="n", yaxt="n")
     lines(c(a.min-da/2,a.min-da/2,a.max+da/2),
           c(a.max+da,a.min-2*da,a.min-2*da))
     text(( a.min+a.max)/2,a.min-2.5*da,
           "True Chronological Age (A)", adj=c(0.5,1),cex=1.25)
     txt = "Indirect\nMeasure\nof Age (a)"
     if(n>1) txt = "Indirect\nMeasures\nof Age (a)"
     text(a.min-da/2.5, a.max+da, txt,
          adj=c(0,1),cex=1.05,font=4)
     for(a in seq(a.min,a.max,da/2)){
       text(a,a.min-2*da,toString(a), adj=c(0.5,1.5))
       segments(a, a.min-2*da, a, a.min-2.05*da)
       text(a.min-da/2,a,toString(a),adj=c(1.5,0.5))
       segments(a.min-da/2, a, a.min-da/2-da/20, a)
     } 
      polygon(c(a.min,a.min,
                a.max+1,a.max+1,a.min),
              c(l.at.a.min,u.at.a.min,
                u.at.a.max,l.at.a.max, l.at.a.min),
              col="grey90",border=NA)
      
      segments(a.min,   a.min,
               a.max+1, beta*(a.max+1),
               col="white",lwd=2)
      
      arrows(a.max+1.2, l.at.a.max,
             a.max+1.2, u.at.a.max,
             code=3,length=0.05,angle=35)
      arrows(a.min-0.2, l.at.a.min,
             a.min-0.2, u.at.a.min,
             code=3,length=0.05,angle=35)
      text(a.min -0.3, a.min, 
           paste( toString(100*conf.level),"%",sep=""),
           adj=c(0.5,0),srt=90,cex=1)
      
      text(a.max+1.3, a.max+1, 
           paste( toString(100*conf.level),"%",sep=""), cex = 1.25,
           adj=c(0,0.5))
      
      UU = 100 ; LL = 0
      
      for (i in 1:n){
          points(a.min-da/2 + i/6, age.hat[i], pch=19,cex=0.5)
          if(conf.level < 1 & i==n & n>1) segments(
               a.min-da/2 + 1/6 , age.bar ,
               a.min-da/2 + n/6 , age.bar ) 
          if(example==1) text(a.min-da/3,age.hat[i],
            toString(round(age.hat[i],1)), cex=0.85,
            adj=c(0,0.5),font=4)
          M = age.hat[i] ; if(conf.level < 1 & n> 1) M = age.bar
          L = M/(1+radius.as.proportion/div)
          R = M/(1-radius.as.proportion/div)
          segments(L, M, R, M, lty="dotted")
          for(a in c(L,M,R)){
             if(example==1 | conf.level < 1) arrows( 
                    a, a.bottom+3.5*dy,
                    a, a.bottom+1.5*dy, 
                    length=0.07,angle=30)
            if(example==1 & a==L){
                
                aA  = toString(round(M,1))
                AA  = toString(round(L,1))
                txt = substitute(paste(
                         "P[ a > ",aA,
                         " | A =",AA, " ] = ", Pval,sep=""),
                          list(aA=aA,AA=AA,Pval=half.alpha))
              arrows( 
                    a, L,
                    a, M,lwd=2,angle=35,length=0.07)
              text(L,L,toString(round(L,1)),adj=c(0.5,1.5))
              text(L,M+0.25,txt,adj=c(0,0.5),srt=75)
            } 
            if(example==1 & a==R){
                aA  = toString(round(M,1))
                AA  = toString(round(R,1))
                txt = substitute(paste(
                         "P[ a < ",aA,
                         " | A =",AA, " ] = ", Pval,sep=""),
                          list(aA=aA,AA=AA,Pval=half.alpha))
                arrows( 
                    a, R,
                    a, M,lwd=2,angle=35,length=0.07,code=2)
                text(R,R,toString(round(R,1)),adj=c(0.5,-0.5))
                text(R,M-0.25,txt,adj=c(0,0.5),srt=-55)
            } 
          }
       if(conf.level ==1 | (conf.level < 1 & i==1 ) ) {
            segments(L,a.bottom + i*dy, R,a.bottom + i*dy,lwd=1.25)
            points(M,a.bottom+i*dy, pch=19,cex=0.5)
            Limits = round(c(L,R),1)
            if(conf.level < 1 | example == 1 ) text(L-0.1,a.bottom + i*dy,
                 toString(Limits[1]), adj=c(1,0.5),cex=0.8,font=2)
            if(conf.level < 1 | example == 1)  text(R+0.1,a.bottom + i*dy,
                 toString(Limits[2]), adj=c(0,0.5),cex=0.8,font=2)
       }
       
       UU = min(UU,R)
       LL = max(LL,L)
     } # i
      
     if(example > 1 & conf.level==1){
       segments(LL, a.bottom, UU,  a.bottom, 
                lwd=3)
       segments(LL, a.bottom+dy, LL,  a.bottom +n*dy,  lty="dotted")
       segments(UU, a.bottom+dy, UU,  a.bottom +n*dy,  lty="dotted")
       Limits = round(c(LL,UU),1)
       text(LL-0.1,a.bottom,toString(Limits[1]), adj=c(1,0.5),cex=0.8,
            font=2)
       text(UU+0.1,a.bottom,toString(Limits[2]), adj=c(0,0.5),cex=0.8,
            font=2)
     } 
     if(example==1 ){ 
         points(age.hat[i],age.hat[i], pch=19,cex=0.75)
         points(age.hat[i],-1.0*dy, pch=19,cex=0.75)
     }
  } 
}



```

Consider first a single indirect measurement of chronological age, that yielded a value of `r round(age.hats[1],1)` years.

Given what you know about the sizes of the possible errors, you **cannot say** that the true age is **exactly** `r round(age.hats[1],1)` years What **can** you say? And with what certainty?

You can put **limits** on the true age by asking **what are the minimum and maximum ages that could have produced the observed reading of**
`r A.hat = round(age.hats[1],1); A.hat`  years.

To do this you need to consider the limits **one scenario at a time**. The **minimum** age that could have given the estimate of `r  A.hat` years is `r Lower = round(age.hats[1],1)` /
`r 1+radius.as.proportion` = 
`r Lower = round(age.hats[1]/(1+radius.as.proportion),1); Lower` years. The **maximum** age that could have produced this reading  is 
`r Upper=round(age.hats[1],1); Upper` / `r 1-radius.as.proportion` = `r round(age.hats[1]/(1-radius.as.proportion),1)` years. Thus (assuming the error model is correct!) you are 100% confident that the true age lies in the interval (`r round(age.hats[1]/(1+radius.as.proportion),1)` , `r round(age.hats[1]/(1-radius.as.proportion),1)`) years. Again, as is clear in the graph, this does not have the typical 
`r A.hat` $\pm$ a single-number margin of error that we typically see in reports. Rather, it is `r A.hat` **- 2.6** and `r A.hat`  **+ 4.4** ! 

But, you can't arrive at these directly; get there this way. You have to try on various limits, until 

$$ LowerLimit + margin  = `r A.hat` \ = \   UpperLimit - margin  $$


```{r,eval=T, echo=F, fig.align="center", warning=FALSE, message=F, fig.cap="100% Confidence Intervals for a person's chronological age when error distributions (that in this example are wider at the  older ages) are 100% confined within the shaded ranges. Left: based on n = 1 measurement; right: based on n = 4 independent measurements. "}

conf.level = 1; 
draw(conf.level)

```

***More data***

The panel on the right shows how, by obtaining 4 independent measurements, and finding the interval they have in common, you can narrow the interval containing the true age.

Can we narrow the interval more, maybe by first averaging the 4 measurements? Should  the mean of 4 measurements  give us more information, ie., a tighter interval, that the one based on the overlap? The sad fact is that, as long we **insist on 100% confidence** in our interval (or our procedure), we can not: the mean of the 3 measurements can still -- theoretically -- be **anywhere** in the same 0.75 $\times$ True Age to 1.25 $\times$ True Age range -- just as a single measurement can.


The **only way to narrow** the interval is to **take a chance, cut corners, and accept a lower confidence level**. To do this, we need to know a bit more about where the pattern (shape) of the error distribution (**up to now we didn't use the _shape_, just the _boundaries_). In other words, we need to know how much of the error distribution is in the corners, so that we can cut them! 

In the next section, we will stick for now with Daniel Bernoulli's error distribution, but cut some corners. (Later on, we will cut some corners on Laplace's and Gauss's error distributions, but with the same standard deviation as in Bernoulli's error curve.)

## More-nuanced intervals


```{r,eval=T, echo=F, fig.align="center", warning=FALSE, message=F}

conf.level = 1; 
q.u = qSemiElliptical(conf.level)


```

We will cut 5% from each corner of the distribution, and focus on the middlemost 90%. From the formula for its mathematical shape, we can calculate that this measurement range is from 
`r paste(-round(q.u,2))` $\times$ the radius of the semi-ellipse to 
`r paste("+",round(q.u,2),sep="")` $\times$ the  radius. There is  only a 5% probability of observing a measurement below (to the left of) this interval, and a 5% probability of observing a measurement above (to the right of) the interval. After we observe our single measurement, we 'try it on' against all possible true-age-scenarios. We retain only those true-age-scenarios in which the observed measurement would fall within this central (90%) range. We discard ('rule out') those age scenarios in which  the measurement would be at one extreme or the other extreme, in one of the two excluded or 'cut' corners.

The left panel of Figure \@ref(fig:ciage2) shows the (now narrower, and more nuanced) **range of true-ages (rahe of parameter values) that is compatible with the observed measurement of 13.1 years**. In all other age-scenarios, the 13.1 would have been too extreme, and so these scenarios are discarded. We can think of the **'ruled in' range** as our (nuanced, compromise) **parameter interval**.  

Note again the method of constructing this **non-symmetric** parameter interval, namely one boundary at a time. It does not fit the $\pm$ mold.

It does, however, give a way of talk about such an interval:

> **The observed measurement (point-estimate) may be an underestimate of the parameter: it might have fallen short of the true parameter value. Or,  it may be an overestimate: it might have overshot the true parameter value. The plus and the minus amounts are the almost-maximal amounts by which our _shot_ might have been off-target.** (as we will see later, the maximal error can be infinite, so we have to put some probalistic limits on the error if we are to narrow the interval).

Q: Does this  **procedure** for constructing intervals  have a 90% success rate, if used up and down all of the ages, say from 10 to 30 years? We could try it out with people of known ages.[^9_6] 

[^9_6]: answer by simulation.

You will  discover in your simulations that it **might matter** whether you simulate the same number of 16 year olds as 10 years, i.e., what the **mix of real ages** is. This does not matter in the 100% intervals, but it might if you are more nuanced. For example,instead of estimating age by an indirect method, pretend you were were **estimating a person's height indirectly**, by just **measuring their arm span** (at each height, the mean armspan is very close to the height, but there is a spread of armspans (pardon the pun!)). And (just like in our example 2 where the spread increases with the mean) the spread of armspans is larger in people who are 6 feet tall than it  is in people 5 feet tall.
BUT, there aren't as many people 5 feet and 6 feet tall as there are people 5 feet 6 inches. So, the distribution of heights in people with a span of 5 feet 11 might have a different shape than that in people with a span of 5 feet 6, or  5 feet. Simulations (or even some diagrams) could settle the issue as to whether the height-mix (or, in example 2, the age mix) matters. What is your intuition as to whether it affects the perfornace of your nuanced parameter estimates? The point is that your method needs to have the same claimed performance (say 90%) at any age you throw at it.


```{r ciage2,eval=T, echo=F, fig.align="center", warning=FALSE, message=F, fig.cap="90% Confidence intervals for Chronological Age when only 90% of the error distributions lie within the shaded ranges."}

conf.level = 0.90; 
draw(conf.level)

```

When we have $n = 3$ observations (right panel), it is not so easy to say how confident we should be about the overlap of the 3 intervals.
Instead, we would be bettter off taking the mean of the 3 measurements, and 'trying on' this single mean against the various sampling distributions of the means of 3 independent measurements from a semi-circular error distribution. Again, since the range remains the same, we would again have to cut corners. 


## Summary

* If an error distribution is bounded, we can be 100% confident in our parameter interval, and we can narrow it by taking more measurements. Moreover, we don't need to specify the exact shape of the error distribution. All that matters are its bounds.

* With unbounded error distributions, a 100% parameter interval may be unacceptably wide, even if we take many measurements. Thus, we have to 'give up something' (in certainty) in order to  'get something' (a narrower interval). Moreover, we need to either (a) specify a model for the shape of the error distribution, or (b) use data-intensive techniques, such as re-sampling, to be able to 'cut the corners.'

* Either way, a logical way to determine parameter intervals is to have them consist of all the parameter-value scenarios in which the observed measurement (or summary measurement) is 'plausible'. The upper limit for the parameter is the scenario in which the measurement would be probalistically near the bottom of the corresponding sampling distribution;  the lower limit is the scenario in which the measurement would be near the top of the corresponding sampling distribution.

* If the error (or sampling) distributions have differing spreads at different parameter values, then the parameter interval will not be symmetric about the point estimate. If the error (or sampling) distributions have the same spreads at different parameter values, then the parameter interval will be symmetric about the point estimate, and thus, easier to calculate.

* It is not correct to view the parameter as 'falling' on one or other side of the measurement. The true parameter values is fixed, and isn't moving or falling anywhere. Rather, it is the observed measurement (point-estimate) that may have fallen to the left of (fallen short of), and thus provided an underestimate of, the true parameter value: Or, it may have overshot the true parameter value, and thus overestimated it. This point also explains why the +/- formula fails us


