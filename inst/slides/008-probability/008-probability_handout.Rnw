\documentclass[10pt,handout]{beamer}


%\input{slides_header.tex}
\input{/home/sahir/git_repositories/EPIB607/slides/slides_header2.tex}
\graphicspath{{/home/sahir/git_repositories/EPIB607/slides/figure/}}

%\let\oldShaded\Shaded
%\let\endoldShaded\endShaded
%\renewenvironment{Shaded}{\footnotesize\oldShaded}{\endoldShaded}

\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Expec}{\operatorname{E}}
\newcommand{\Prob}{\operatorname{P}}


\begin{document}

<<setup, include=FALSE>>=
library(knitr)
knitr::opts_chunk$set(cache=FALSE, message = FALSE, tidy = FALSE, warning = FALSE,
echo = FALSE, 
#fig.width = 8, 
#fig.asp = 0.8, 
fig.align = 'center', 
#out.width = "0.50\\linewidth", 
size = 'tiny')

# the kframe environment does not work with allowframebreaks, so remove it
#knit_hooks$set(document = function(x) {
#gsub('\\\\(begin|end)\\{kframe\\}', '', x)
#})

library(tidyverse)
library(NCStats)
#options(digits = )


#knitr::opts_chunk$set(background = '#FFFF00')
library(tools) #needed for include_graphics2 function

pacman::p_load(here)

source(here::here("inst","slides","bin","include_graphics2.R"))

knitr::knit_hooks$set(purl = hook_purl)

pacman::p_load(
gapminder,
here,
socviz, 
tidyverse,
kableExtra,
DT
)

theme_set(cowplot::theme_cowplot())
@

%\title{Introduction to Regression Trees}
%\author{Sahir Bhatnagar \inst{1}}
%\author[shortname]{Sahir Rai Bhatnagar, PhD Candidate (Biostatistics) }
%\institute[shortinst]{Department of Epidemiology, Biostatistics and Occupational Health}

\title{008 - Probability}
\author{EPIB 607}
\institute{
	Sahir Rai Bhatnagar\\
	Department of Epidemiology, Biostatistics, and Occupational Health\\
	McGill University\\
	
	\vspace{0.1 in}
	
	\texttt{sahir.bhatnagar@mcgill.ca}\\
	\texttt{\url{https://sahirbhatnagar.com/EPIB607/ChapProbability.html}}
}

\date{slides compiled on \today}

\maketitle

\section{Introduction to probability}

\begin{frame}{Introduction}
	
	\begin{itemize}
		\item This section extends the notion of variability that was introduced in
		the context of data to other situations. 
		\item The variability of the entire \textbf{population} and the concept of a \textbf{random variable} is discussed. 
		\item These concepts are central for the development and interpretation of
		statistical inference. 
	\end{itemize}
	
\end{frame}



\begin{frame}{Objectives}
	
	\begin{itemize}[<+->]
		
		\item    Consider the distribution of a variable in a population and compute
		parameters of this distribution, such as the mean and the standard
		deviation.
		
		\item   Become familiar with the concept of a random variable.
		
		\item    Understand the relation between the distribution of the population
		and the distribution of a random variable produced by sampling a
		random subject from the population.
		
		\item    Identify the distribution of the random variable in simple settings
		and compute its expectation and variance.
	\end{itemize}
	
\end{frame}




\begin{frame}[fragile]{Variability in Data}
	\begin{columns}
		\begin{column}{0.5\textwidth}  %%<--- here
			<<echo=FALSE, fig.asp=0.681, message=FALSE, fig.cap='A boxplot and dot plot of \\texttt{clutch.volume} in the \\texttt{frog} dataset from the \\texttt{oibiostat} package. The horizontal dashes indicate the bottom 50\\% of the data and the open circles represent the top 50\\%.'>>=
library(oibiostat)
library(openintro)
data(COL)
data(frog)

d   <- frog$clutch.volume

par(  mar = c(0, 4, 0, 1),
mgp = c(2.8, 0.7, 0))
boxPlot(d,
ylab = 'Clutch Volume',
xlim = c(0.3, 3),
axes = FALSE,
ylim = range(d))
axis(2)

arrows(2,min(d), 1.4, min(d) - 5, length = 0.08)
text(2, min(d), 'lower whisker', pos = 4)

q1 <- quantile(d, 0.25)
arrows(2, q1, 1.4, q1, length = 0.08)
text(2, q1,
expression(Q[1]~~'(first quartile)'), pos = 4)

m <- median(d)
arrows(2, m, 1.4, m, length = 0.08)
text(2, m, 'median (second quartile)', pos = 4)

q3 <- quantile(d, 0.75)
arrows(2, q3, 1.4, q3, length = 0.08)
text(2, q3,
expression(Q[3]~~'(third quartile)'), pos = 4)

arrows(2, rev(sort(d))[11],
1.4, rev(sort(d))[11], length = 0.08)
text(2, rev(sort(d))[11],
'upper whisker', pos = 4)

#y <- quantile(d, 0.75) + 1.5 * IQR(d)
#arrows(2, y, 1.4, y, length = 0.08)
#lines(c(0.72, 1.28), rep(y, 2),
#lty = 3, col = '#00000066')
#text(2, y,
#'max whisker reach', pos = 4)

m <- rev(tail(sort(d), 9))
s <- m[1] - 0.4 * sd(m)
arrows(2, s - 200, 1.1, m[1] - 0.2, length = 0.08)
arrows(2, s - 200, 1.1, m[2] + 0.3, length = 0.08)
arrows(2, s - 200, 1.1, m[3] + 0.35, length = 0.08)
arrows(2, s - 200, 1.1, m[4] + 0.4, length = 0.08)
arrows(2, s - 200, 1.1, m[5] + 0.45, length = 0.08)
arrows(2, s - 200, 1.1, m[6] + 0.5, length = 0.08)
#arrows(2, s - 200, 1.1, m[7] + 0.55, length = 0.08)
arrows(2, s - 200, 1.1, m[8] + 0.6, length = 0.08)
arrows(2, s - 200, 1.1, m[9] + 0.7, length = 0.08)
text(2, s - 200, 'outliers', pos = 4)

points(rep(0.4, 215), rev(sort(d))[1:215],
cex = rep(1.3, 431),
col = rep(COL[1, 3], 431),
pch = rep(1, 431))
points(rep(0.4, 215), sort(d)[1:215],
cex = rep(1, 215),
col = rep(COL[4,3], 215),
pch = rep("-", 215))
			@
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item We previously examined the variability in data. 
				\item In the statistical context, data is obtained by selecting a sample from the
				target population and measuring the quantities of interest for the
				subjects that belong to the sample. 
				\item Different subjects in the sample may obtain different values for the measurement, leading to variability in
				the data.
				\item Numerical summaries may be computed in order to characterize the main
				features of the variability, e.g., mean, median, sample variance, sample standard deviation, inter-quartile range
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}



\begin{frame}{Two other forms of variability}
	
	\begin{itemize}
		
		\item  The subject of this section is to introduce two other forms of
		variability, variability that is not associated, at least not directly,
		with the data that we observe. \pause
		\begin{enumerate}
			\item Population variability
			\item Variability of a random variable
		\end{enumerate}

	\pause
		
		\item The notions of variability that will be presented are abstract, they are
		not given in terms of the data that we observe, and they have a
		mathematical-theoretical flavor to them. 
		
		\pause
		
		\item At first, these abstract
		notions may look to you as a waste of your time and may seem to be
		unrelated to the subject matter of the course. 
		
		\pause
		
		\item The opposite is true. The
		very core of statistical thinking is relating observed data to
		theoretical and abstract models of a phenomena. 
		
		\pause 
		\item The abstract notions of variability that are introduced in this
		chapter, and are extended in the subsequent chapters, are the essential foundations for the practice of
		statistics
	\end{itemize}
	
\end{frame}


\begin{frame}{Population variability}
	
	\begin{itemize}[<+->]
		
		\item  We will examine the data set \texttt{heights\_sample.csv} available at \url{https://github.com/sahirbhatnagar/EPIB607/blob/master/inst/data/heights_sample.csv} which contains data on the sex and height of a sample of 100 observations. 
		\item We will also consider the sex and height of all the members of the population from which the sample was selected available at \url{https://github.com/sahirbhatnagar/EPIB607/blob/master/inst/data/heights_population.csv}
		\item The size of the relevant population is 100,000, including the 100
		subjects that composed the sample. 
		\item When we examine the values of the
		height across the entire population we can see that different people may
		have different heights. This variability of the heights is the \textbf{population variability}
	\end{itemize}
	
\end{frame}


\begin{frame}{Variability of a random variable}
	
	\begin{itemize}[<+->]
		
		\item  The other abstract type of variability, the \textbf{variability of a random variable}, is a mathematical concept. 
		\item The aim of this concept is to model the notion of randomness in measurements or the uncertainty regarding
		the outcome of a measurement. 
		\item In particular we will initially consider the variability of a random variable in the context of selecting one
		subject at random from the population.
		\item Random variables provide models for randomness and uncertainty in measurements. 
		\item Simple examples of such abstract random variables will be provided in this
		chapter. More examples will be introduced in the subsequent chapters.
	\end{itemize}
	
\end{frame}


\begin{frame}{A point to remember}
	
	\begin{itemize}[<+->]
		
		\item The variability of the data relates to a concrete list of data values that is presented to us. 
		\item The other types of variability are
		not associated with quantities we actually get to observe. 
		\item  The data for
		the sample we get to see but not the data for the rest of the
		population. 
		\item Yet, we can still discuss the variability of a population
		that is out there, even though we do not observe the list of
		measurements for the entire population. 
		\item The discussion of the variability in this context
		is theoretical in its nature. Still, this theoretical discussion is
		instrumental for understanding statistics.
	\end{itemize}
	
\end{frame}



\section{Variability of a population}


\begin{frame}[fragile]{Sample of heights}

<<echo=TRUE, size = 'scriptsize'>>=
library(dplyr)
library(rio)

heights_sample <- rio::import(
                   here::here("inst/data/heights_sample.csv"))
heights_sample <- heights_sample %>% 
                    dplyr::mutate(sex = factor(sex))

summary(heights_sample)

heights_sample %>%
   dplyr::glimpse()
@

\end{frame}


\begin{frame}[fragile]{Population of heights}
	
	<<echo=TRUE, size = 'scriptsize'>>=
heights_population <- rio::import(
                 here::here("inst/data/heights_population.csv"))
heights_population <- heights_population %>% 
                    dplyr::mutate(sex = factor(sex))
                    
summary(heights_population)
heights_population %>%
   dplyr::glimpse()
	@
	
\end{frame}


\begin{frame}[fragile]{Distribution of population of heights}
	
	<<echo=TRUE, size = 'scriptsize', fig.asp = 0.621>>=
library(ggplot2); library(cowplot)
ggplot2::theme_set(cowplot::theme_cowplot())
p <- ggplot(data = heights_population, 
             mapping = aes(x = height))
p + geom_bar()
	@
	
\end{frame}



\begin{frame}{Population mean $\mu$}
	
	\begin{itemize}[<+->]
		
		\item The formula of the population mean is:
		
		$$\mu = \frac{\mbox{Sum of all values in the population}}{\mbox{Number of values in the population}}= \frac{\sum_{i=1}^N y_i}{N}\;.$$
		
		
		\item Observe the similarity between the definition of the mean for the data
		and the definition of the mean for the population. In both cases the
		arithmetic average is computed. The only difference is that in the case
		of the mean of the data the computation is with respect to the values
		that appear in the sample whereas for the population all the values in
		the population participate in the computation.
		
		\item In actual life, we will not have all the values of a variable in the
		entire population. Hence, we will not be able to compute the actual
		value of the population mean. 
		
		\item It's still meaningful to talk about the population mean because this number exists, even though we do
		not know what its value is. Statistics is about trying to estimate this unknown quantity on the basis of the data we do have in the sample.
		
	\end{itemize}
	
\end{frame}


\begin{frame}[fragile]{Population variance $\sigma^2$}
	
	\begin{itemize}[<+->]
		
		\item The formula of the population variance is defined in a similar way:
		
$$
\begin{aligned}
	\sigma^2 =& \mbox{The average square deviation in the population}\\
	=& \frac{\mbox{Sum of the squares of the deviations in the population}}{\mbox{Number of values in the population}}\\
	=& \frac{\sum_{i=1}^N (y_i-\mu)^2}{N}\;.
\end{aligned}
$$
		
		\item In \texttt{R}:
		
		<<echo=TRUE, size = 'scriptsize'>>=
		N <- nrow(heights_population)
		var(heights_population$height) * (N - 1) / N
		@
		
		\item The standard deviation of the population, yet another parameter, is
		denoted by $\sigma$ and is equal to the square root of the variance. 
		
		\item The typical situation is that we do not
		know what the actual value of $\sigma$. Yet, we may refer to it as a quantity and we may try to estimate its
		value based on the data we do have from the sample.
		
		

		
	\end{itemize}
	
\end{frame}


\section{Variability of a random variable}


\begin{frame}[fragile]{Variability of a random variable}
	
	\begin{itemize}[<+->]
		
		\item As an example, consider taking a sample of size $n=1$ from the population (a single
		person) and measuring his/her height. We will apply the function \texttt{dplyr::sample\_n} to sample one row from the data frame:
		
		<<echo = TRUE>>=
		heights_population %>% 
		   dplyr::sample_n(size = 1)
		@
		
		\item Let us run the function again:
		
		<<echo = TRUE>>=
heights_population %>% 
dplyr::sample_n(size = 1)
@

\item And again

		<<echo = TRUE>>=
heights_population %>% 
dplyr::sample_n(size = 1)
@
		
		
	\end{itemize}
	
\end{frame}


\begin{frame}[fragile]{Random variable}
	\small
	\begin{itemize}[<+->]
		
		\item A random variable is the future outcome of a measurement, \textbf{before}
		the measurement is taken. It does not have a specific value, but rather
		a collection of potential values with a distribution over these values.
		
		\item After the measurement is taken and the specific value is revealed then
		the random variable ceases to be a random variable! Instead, it becomes
		data.
		
		\item One is not able to say what the outcome of a random variable
		will turn out to be. 
		
		\item However, one may identify patterns in this potential
		outcome. For example, knowing that the distribution of heights in the
		population ranges between 117 and 217 centimeter one may say in advance
		that the outcome of the measurement must also be in that interval.
		
		\item Since there is a total of 3,476 subjects with height equal to
		168 centimeters and since the likelihood of each subject to be selected
		is equal then the likelihood of selecting a subject of this height is
		3,476/100,000 = 0.03476. In the context of random variables we call this
		likelihood probability. 
		
		\item In the same vain, the frequency of subjects
		with hight 192 centimeter is 488, and therefore the probability of
		measuring such a height is 0.00488. The frequency of subjects with
		height 200 centimeter or above is 393, hence the probability of
		obtaining a measurement in the range between 200 and 217 centimeter is
		0.00393.
		
		
	\end{itemize}
	
\end{frame}


\begin{frame}[fragile]{Sample Space and Distribution}
	\small
	\begin{itemize}[<+->]
		
		\item A random variable refer to numerical values, typically the outcome of an
		observation, a measurement, or a function thereof.
		
		\item A random variable is characterized via the collection of potential
		values it may obtain, known as the \textbf{sample space} and the likelihood of
		obtaining each of the values in the sample space (namely, the
		probability of the value).
		
		\item The probability of each value is the height
		of the bar above the value, divided by the total frequency of 100,000
		(namely, the relative frequency in the population).
		
		\item We will denote random variables with capital Latin letters such as $X$,
		$Y$, and $Z$. Values they may obtain will be marked by small Latin
		letters such as $x$, $y$, $z$. For the probability of values we will use
		the letter “$\Prob$". Hence, if we denote by $Y$ the measurement of
		height of a random individual that is sampled from the given population
		then: 
		
		$$\Prob(Y = 168) = 0.03476$$ and 
		$$\Prob(Y \geq 200) = 0.00393\;.$$
		
	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{Sample Space and Distribution}
	\small
	\begin{itemize}[<+->]
		
		\item Consider, as yet another example, the probability that the height of a
		random person sampled from the population differs from 170 centimeter by
		no more than 10 centimeters. (In other words, that the height is between
		160 and 180 centimeters.) Denote by $Y$ the height of that random
		person. We are interested in the probability
		$\Prob(|Y -170| \leq 10)$
		
		\item In \texttt{R}:
		
		<<echo = TRUE, size = 'normalsize'>>=
		Y <- heights_population$height
		
		mean(abs(Y-170) <= 10)
		@
		
	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{A note on the previous calculation}
	\small
	\begin{itemize}[<+->]
		
		\item Let us produce a small example that will help us explain the computation
		of the probability. We start by forming a sequence with 10 numbers:
				
		<<echo = TRUE, size = 'normalsize'>>=
		Y <- c(6.3, 6.9, 6.6, 3.4, 5.5, 4.3, 6.5, 4.7, 6.1, 5.3)
		@
		
		The goal is to compute the proportion of numbers that are in the range
		$[4,6]$ (or, equivalently, $\{|Y-5| \leq 1\}$).
		
		\item 
		
<<echo = TRUE, size = 'normalsize'>>=
abs(Y-5)
@

\item 

<<echo = TRUE, size = 'normalsize'>>=
abs(Y - 5) <= 1
@


\item 		
<<echo = TRUE, size = 'normalsize'>>=
mean(abs(Y - 5) <= 1)
@
		
	\end{itemize}
	
\end{frame}




\begin{frame}[fragile]{Probability function}
	\small
	\begin{itemize}[<+->]
		
		\item The probability function of a random variable is defined for any value
		that the random variable may obtain and produces the \textbf{distribution} of
		the random variable. The probability function may emerge as a relative
		frequency as in the given example or it may be a result of theoretical
		modeling.
		
		\item Consider the following probability distribution:
		
		<<echo=FALSE, size = 'small'>>=
		tab1 = data.frame(Value=c(0,1,2,3),Probability=c(.5,.25,.15,.1),Cum.Prob=c(.5,.75,.9,1))
		knitr::kable(
		tab1, 
		booktabs = TRUE
		)
		@
		
		\item What is $\Prob(Y=0)$, the probability that $Y$ is equal to 0?:
		
		\item What is the probability of $Y$ falling in the interval $[0.5, 2.3]$ ?
		
		
	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{Expectation}
	\small
	\begin{itemize}[<+->]
		
		\item We may characterize the center of the distribution of a random variable
		and the spread of the distribution in ways similar to those used for the
		characterization of the distribution of data and the distribution of a
		population.
		
		\item The \textbf{expectation} marks the center of the distribution of a random
		variable. It is equivalent to the data average $\bar y$ and the
		population average $\mu$, which was used in order to mark the location
		of the distribution of the data and the population, respectively.
		

	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{Expectation}
	\small
	\begin{itemize}[<+->]
		
		\item The average of the data can be computed as the weighted average of the values that are present
		in the data, with weights given by the relative frequency. Specifically,
		for the data
		
		$$1,\; 1,\; 1,\; 2,\; 2,\; 3,\; 4,\; 4,\; 4,\; 4,\; 4,$$ the mean can be calculated via
		
		\begin{align*}
			\bar{y} &= \frac{1 + 1 + 1 + 2 + 2 + 3 + 4 + 4 + 4 + 4 + 4}{11}\\
			& =	1\times \frac{3}{11} + 2 \times \frac{2}{11} + 3 \times \frac{1}{11} + 4 \times \frac{5}{11}
		\end{align*}

		producing the value of $\bar y =2.727$ in both representations. 
		
		\item Using a formula, the equality between the two ways of computing the mean is
		given in terms of the equation:
		
		$$\bar y = \frac{\sum_{i=1}^n y_i}{n} = \sum_y \big(y \times (f_y/n)\big)\;,$$
		where $f_y/n$ represents the frequency of $y$ in the data. 
		

		
		
	\end{itemize}
	
\end{frame}


\begin{frame}[fragile]{Expectation}
	\small
	\begin{itemize}		
	
		\item Using a formula, the equality between the two ways of computing the mean is
		given in terms of the equation:
		
		$$\bar y = \frac{\sum_{i=1}^n y_i}{n} = \sum_y \big(y \times (f_y/n)\big)\;,$$
		where $f_y/n$ represents the frequency of $y$ in the data. 
		
		\item The expectation of a random variable is computed in the spirit of the
		second formulation, and is define via the equation:
		
		$$\Expec(Y) = \sum_y \big(y \times \Prob(y)\big)\;.$$
		
		
	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{Variance}
	\small
	\begin{itemize}		
		
		\item The sample variance ($s^2$) is obtained as the sum of the squared
		deviations from the average, divided by the sample size ($n$) minus 1:
		
		$$s^2 = \frac{\sum_{i=1}^n (y_i - \bar y)^2}{n-1}\;.$$ 
		
		\item  A second	formulation for the computation of the same quantity is via the use of
		relative frequencies. The formula for the sample variance takes the form
		
		$$s^2 = \frac{n}{n-1}\sum_y \big((y - \bar y)^2\times (f_y/n)\big)\;.$$
		
		
		\item In a similar way, the variance of a random variable may be defined via
		the deviation from the expectation. This deviation is
		then squared and multiplied by the probability of the value. The
		multiplications are summed up in order to produce the variance:
		
		$$\Var(Y) = \sum_y\big( (y-\Expec(Y))^2 \times \Prob(y)\big)\;.$$
		
	\end{itemize}
	
\end{frame}


	\begin{frame}[fragile]{Session Info}
	\tiny
	
	<<echo=FALSE, comment = NA, size = 'tiny'>>=
	print(sessionInfo(), locale = FALSE)
	@
	
\end{frame}

\end{document}

































\begin{frame}{The (statistical) parameters we will be concerned with: $\mu$}
	

\begin{itemize}
	\item $\mu$: The mean level of a quantitative characteristic, e.g. the depth of the earth's ocean or height of the land, or the height / BMI / blood pressure levels of a human population. \pause 
	\item One could also think of mathematical and physical constants as parameters, even though their values are effectively 'known.' Examples where there is agreement to many many decimal places include the mathematical constant pi, the speed of light (c), and the gravitational constant G. The speed of sound depends on the medium it is travelling through, and the temperature of the medium. The freezing and boiling points of substances such as water and milk depend on altitude and barometric pressure.  \pause 
	\item At a lower level, we might be interested in personal characters, such as the size of a person's vocabulary, or a person's mean (or minimum, or typical) reaction time. The  target could be a person's 'true score' on some test -- the  value one would get if one (could, but not realistically) be tested on each of the (very large) number of test items in the test bank, or observed/measured continously over the period of interest.  

\end{itemize}

\end{frame}



\begin{frame}{The (statistical) parameters we will be concerned with: $\pi$}
	
	
	\begin{itemize}
	
		\item $\pi$: Prevalence or risk (proportion): e.g.,  proportion of the earth's surface that is covered by water, or of a human population that has untreated hypertension, or lacks internet access, or will develop a new health condition over the next x years. \pause 
		\item At a lower level, we might be interested in personal proportions, such as what proportion of the calories a person consumes come from fat, or the proportion of the year 2020 the person spent on the internet, or indoors, or asleep, or sedentary.	
	\end{itemize}
	
\end{frame}





\begin{frame}{The (statistical) parameters we will be concerned with: $\lambda$}
	
	
	\begin{itemize}
			
		\item $\lambda$: The speed with which events occur: e.g., earthquakes per earth-day, or heart attacks or traffic fatalities per (population)-year. 
		\item At a lower level, we might be interested in personal intensities, such as the mean number of tweets/waking-hour  a person issued during the year 2020, or the mean number of times per 100 hours of use a person's laptop froze and needed to be re-booted.
		
	\end{itemize}
	
\end{frame}



\begin{frame}{The (statistical) parameters we will be concerned with}
	
	
	\begin{itemize}
		\item Each of these three parameters refers to a characteristic of the overall domain, such as entire surface of the earth, or the entire ocean, or population. There are no indicators for distinguishing among subdomains, so they refer to locations / persons not otherwise specified. We will drill down  later. \pause 
		
		\item Especially for epidemiologic research, and also more generally, one can think  of $\pi$ and $\lambda$ as  parameters of occurrence. [Although the word occurrence usually has a time element, it can also be timeless: how frequently a word occurs in a static text, or a mineral in a rock.] \pause 
		
		\item Prevalence is the proportion in a current state, and the 5-year risk is the expected proportion or probability of being in a new state 5 years from now. The  parameter $\lambda$ measures the speed with which the  elements in question move from the original to the other state.\pause 
		
		\item Even though the depths of the ocean, and blood pressures, are measured on a quantitative (rather than on all or none) scale, one can divide the scale into a finite number of bins/caterories, and speak of the prevalence (proportion) in each category. Conversely, one can use a set of descriptive parameters called quantiles, i.e, landmarks such that selected proportions, e.g., 0.05 or 5\%, 25\%, 50\%, 75\%, 95\% of the distribution are to the left of ('below') these quantiles.
	\end{itemize}
	
\end{frame}



\begin{frame}{Terminology}
\begin{itemize}
	\item Before we go on, we need to adopt sensible terminology for referring generically to the states, traits, conditions or behaviours  whose category-specific parameter values are being compared. \pause 
	
	\item We will use the term  \textbf{determinant}. It has several advantages over the many other terms used in different disciplines, such as exposure, agent, independent/explanatory variable, experimental condition, treatment, intervention, factor, risk factor, predictor. The main advantage is that it is broader, and closer to causally neutral in its connotaion. \pause 
	
	\item  \textit{Exposure} has environmental connotations, and technically refers to an opportity to injest or mentally take on board a substance or message.  
	\pause 
	\item The term \textit{independent variable} suggests the investigator has control over it in a laboratory setting. 
	\pause 
	\item The term \textit{explanatory} is  ambiguous as to the mechanism by which the parameter value in the index category got to be different from the value in the index category. Not all contrasts are experimentally formed. 

\end{itemize}
\end{frame}


\begin{frame}{Terminology}
	\begin{itemize}
		\item The term \textit{factor}, and thus the term \textit{risk factor}, are to be avoided because the word factor derives from the Latin \textit{facere}, (the action of) doing, making, creating. 
		\pause 		
		\item \textit{Predictor} makes one think of the future. 
		
		\pause 
		\item The term \textit{regressor} (or its shorthand, the 'X' ) won't be understood by lay people.
		
				\pause 
		\item While the word \textbf{determine} can suggest causality (e.g., demand determines the price), it also refers to 'fixing the form, position, or character of beforehand': two points determine a straight line; the area of a circle is determined by its radius.
		
				\pause 
		\item We now move on to the parameter relations we will be concerned with, beginning with the simplest type.
	\end{itemize}
\end{frame}



\begin{frame}{Parameter Contrasts}
	\begin{itemize}
		\item In applied research, we are seldom interested in a single constant. 
		
		
				\pause 
		
		\item Much more often we are interested in the contrast (difference) between the parameter values in different contexts/locations (Northern hemisphere vs Southern hemisphere), conditions/times (reaction times using the right versus left hand, or behaviour on weekdays versus weekends), or sub-domains or sub-populations (females vs males).
		
		\pause 
		
		\item In this section, we will limit our attention to 'contrasts': a compariosn of the parameter values between 2 contexts/locations/sub-populations. Thus the parameter function has just 2 possible 'input' values. 
	\end{itemize}
\end{frame}



\begin{frame}{'Reference' and 'Index' categories}
	\begin{itemize}
		\item In many research contexts, the choice of 'reference' category (starting point, the category against which the other category is compared) will be obvious: it is the \textbf{status quo} (standard care, prevailing condition or usual situation, dominant hand, better known category). 
		\pause 
		\item The 'index' category is \textbf{the category one is curious about} and wishes to learn more about, by contrasting its parameter value with the parameter value for the reference category.
		\pause 		
		\item In other contexts, it is less obvious which category should serve as the reference and the index categories, and the choice may be merely a matter of persepctive. If one is more famiar with the Northern hemisphere, it serves as a natural starting point 
		
				\pause 
		\item The choice of reference category in a longevity contrast between males and females, or in-hospital mortality rates or motor vehicle fatality rates during weekends versus weekdays, might depend on what mechanism one wishes to bring out. 
		
				\pause 
		\item Or one might chose as the reference category the one with the larger amount of experience, or maybe the one with the lower parameter value, so that the 'index minus reference' difference would be a positive quantity, or the 'index: reference ratio' exceeds 1.
	\end{itemize}
\end{frame}




\begin{frame}{'Reference' and 'Index' categories}
	\begin{itemize}
		\item The choice of reference category in a longevity contrast between males and females, or in-hospital mortality rates or motor vehicle fatality rates during weekends versus weekdays, might depend on what mechanism one wishes to bring out. 
		
		\item Or one might chose as the reference category the one with the larger amount of experience, or maybe the one with the lower parameter value, so that the 'index minus reference' difference would be a positive quantity, or the 'index: reference ratio' exceeds 1.
	\end{itemize}
\end{frame}



\begin{frame}{Parameter relations in numbers and words}
\begin{itemize}
\item To make this concrete, we will use hypothetical (and very round) numbers and pretend we 'know' the true parameter values -- in our example
of the mean depth of the ocean in the Northern hemisphere (reference category) and Southern hemisphere (index category) -- to be 3,600 metres (3.6Km) and 4,500 metres (4.5Km) respectively. Thus, the difference (South minus North) is 900 metres or 0.9Km.
\pause
\item If we wished to show the two parameter values graphically, we might do so using the format in panel (a), which shows the 2 hemisphere-specific parameter values -- but forces the reader to calculate the difference.

\item Panel (b) follows a more reader-friendy format, where the difference (the quantity of interest) is isolated:  the original 2 parameters are converted to 2 new, more relevant ones. 

\item Panel (c) encodes the relation displayed in panel (b) in a \textbf{single phrase} that applies to \textbf{both} categories: Onto the 'starting value' of 3.8Km, one \textbf{adds} $\Delta \mu$ = 0.9 Km \textbf{only if} the  resulting parameter pertains to the Southern hemisphere. The 0.9 Km is toggled off/on as one moves from North to South.   
\end{itemize}
\end{frame}




\begin{frame}[fragile,plain]

<<eval=T, echo=F, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F>>=
	plot3 = function(row){
		par(mfrow=c(1,3),mar = c(0,0,0,0.5) )
		TOP="Mean Ocean\ndepth (Km)"
		x=c(0.225,0.775);; dx=0.025
		mu = c(3.6,4.5)
		
		LABEL=c("North", "South")
		delta.mu = mu[2]-mu[1]
		
		PARA = t(matrix( c( toString(mu[1]),
		toString(mu[2]),
		toString(mu[1]), 
		paste( toString(mu[1]), " + ",
		toString(delta.mu),sep=""),
		rep(paste(toString(mu[1]),
		" + ",toString(delta.mu),
		" if South",sep=""),2)
		),2,3) )
		
		PARA.S = c( expression(mu[North]), 
		expression(mu[South]),
		expression(mu[North]),
		expression(mu[North] + Delta * mu),
		expression(mu[S] == paste(mu[0] + Delta * mu ,
		" ",textstyle(x)," S")),
		expression(mu[S] == paste(mu[0] + Delta * mu ,
		" ",textstyle(x)," S")),
		
		expression(mu[North]), 
		expression(mu[South]),
		expression(mu[North]),
		expression(mu[North] %*% Delta * mu),
		
		expression(mu[S] == paste(mu[0] %*% Delta * mu ,
		" ",textstyle(x)," S")),
		expression(mu[S] == paste(mu[0] %*% Delta * mu ,
		" ",textstyle(x)," S"))
		
		)
		
		PARA.R = c( toString(mu[1]),
		toString(mu[2]),
		toString(mu[1]),
		expression(3.6 %*% frac(4.5, 3.6) ),
		rep(expression(
		paste(3.6 %*% frac(4.5, 3.6)," if South")
		), 2)
		)
		
		PARA.RS = c(  expression(mu[North]), 
		expression(mu[South]),
		expression(mu[North]),
		expression(mu[North] %*% frac(mu[South], mu[North]) ),
		rep(expression(mu[S] == mu[0]
		%*% Ratio^s), 2)
		)
		
		SRT=38 
		
		dy= 1
		for(co in 1:3){
			plot(x,mu,yaxt="n",xlim=c(0,1),
			ylim=c(-0.2,ceiling(mu[2])))
			text(0, ceiling(mu[2]),
			c("(a)","(b)","(c)")[co],
			adj=c(0,0), cex=2)
			
			rect(x-dx,0,x+dx,mu,
			col="lightblue",border=NA)
			if(co>=2){
				points(x[2],mu[1],cex=1)
				polygon( c(x,x[2]), c(mu[1],mu),
				col=NA,border="grey50",lwd=0.75)
			}
			
			if(co==1){
				arrows(x[1]-0.1,0,
				x[1]-0.1,ceiling(mu[2]),
				length=0.07,angle=30)
				text(x[1]-0.15, mean(mu),
				"Mean Depth of Oceans (Km)",
				adj=c(1.25,0), srt=90, cex=2.0)
			}
			for( y in seq(0,ceiling(mu[2]),dy)){
				segments(x[1], y, x[2],y, col="grey50",lty="dotted")
				if (co == 1 & y %in% c(0,1,2,3,5) ) text(x[1]-dx, y,toString(y),
				adj=c(1,0.5),cex=1.5)
				if(co == 3 & y %in% c(0,1,2,3,4)  ) text(x[2]+dx, y,toString(y), 
				adj=c(0,0.5), cex=1.5)
				
			} 
			text(0.5,1.05*ceiling(mu[2]),TOP,adj=c(0.5,0),cex=1.2,font=2) 
			points(x,mu,pch=19,cex=0.75)
			for(j in 1:2){
				txt = PARA[co,j]
				if(row==2) txt = PARA.S[(co-1)*2+j]
				if(row==3) txt = PARA.R[(co-1)*2+j]
				if(row==4) txt = PARA.RS[(co-1)*2+j]
				
				xx = x[j]  - 0.00*(j==1) + 0.00*(j==2)
				if(co==3) xx = mean(x)
				
				yy = mu[j] + 0.15 + 0.05*(row >= 3)
				if(co==3) yy =  mean(mu) +  0.15 + 
				0.15*(row >= 3)
				
				text(xx, yy, txt,  adj=c(0.6, 0.5),
				cex=1.75,srt=SRT) 
				
				if(row==4 & co==3 & j==1) text(
				mean(x), 2.5,
				expression(Ratio == frac(mu[South], mu[North] ) ),
				cex=1.75)
				
				LAB = LABEL[j]
				if(row %in% c(2,4) & co == 3 ) LAB=c("0","1")[j]
				text(x[j],-0.15,LAB, adj=c(0.5,1),
				cex=1.5)
				if(row %in% c(2,4) & co == 3 & j==1 ) text( 
				mean(x),-0.25,"S", adj=c(0.5,1),cex=1.5)
				
			} 
		}
	}
	
	
	plot3(1)
	
@
\end{frame}


\begin{frame}{Parameter relations in symbols, and with the help of an  index-category indicator}
\begin{itemize}
\item Panels (a) and (b) in the following figure repeat the information in panels (a) and (b) in the preceding Figure, but using Greek letters to symbolically represent the parameters. Just to keep the graphics uncluttered, the labels North and South are abbreviated to N and S and used as subscripts. Also, for brevity, the expression $\Delta \mu$ denotes $\mu_S - \mu_N$. 

\pause 

\item The relation encoded in a single phrase shown in the previous panel (c) has a compact form suitable for verbal communication. The representation can be adapted to be more suitable for computer calculations. (The benefit of doing this will become obvious as soon as you try to learn the parameter values by fiiting these models to actual data.) Depending on whether the hemisphere in question is the northern or southern hemisphere, the expression/statement 'the specified hemisphere is the SOUTHERN hemisphere' evaluates to a (logical) FALSE or TRUE. In the binary coding used in computers, it evaluates to 0 or 1, and we call such a 0/1 variable an 'indicator' variable.



\end{itemize}	
	
\end{frame}




\begin{frame}{Parameter relations in symbols, and with the help of an  index-category indicator}
	\begin{itemize}
		\item In panel (c) in the following figure, just to keep the graphics uncluttered, the name of the indicator variable SOUTHERN is abbreviated to S, and $\mu_S$ is shorthand for the $\mu$ cooresponding to whichever value (0 or 1) of $S$ is specified (we could also write it as $\mu | S$, or $\mu$ 'given' $S$.) Thus, the symbol $\mu_0$ refers to the $\mu$ when $S=0$, or in longerhand, to $\mu \ | \ S = 0$. 
		
	\end{itemize}	
	
\end{frame}



\begin{frame}[fragile,plain]
<<eval=T, echo=F, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F>>=
plot3(2)
@
\end{frame}




\begin{frame}{Relative differences (ratios) - in numbers}
\begin{itemize}
	\item A ratio can be more helpful than a difference, especially if you are don't have a sense of how large the parameter value is even in the reference category. As an example, on average, how many more red blood cells do men have than women? or how much faster  are gamers' reaction times compared with nongamers?
\pause 
\item Recall our hypothetical mean ocean depths, 3.6 Km  in the 
oceans in the Northern hemisphere (reference category) and 4.5Km in the oceans of the Southern hemisphere (index category). Thus, the S:N (South divided by North) ratio is 4.5/3.6 or 1.25.
\pause 
\item Panel (a) leaves it to the reader to calculate the ratio of the parameter values. In panel (b)  the ratio (the quantity of interest) is isolated:  again, the original 2 parameters are converted to 2 new, more relevant ones. 
\pause 
\item Again, panel (c) shows a single master-equation that applies to both hemispheres by togging off/on the ratio of 4.5/3.6.
\end{itemize}
\end{frame}


\begin{frame}[fragile,plain]
	<<eval=T, echo=F, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F>>=
	plot3(3)
	@
\end{frame}





\begin{frame}{Relative differences (ratios) - expressed in symbols}
	\begin{itemize}
		\item To rewrite these numbers in a symbolic equation suitable for a computer, we  again convert the logical 'if South' to a numerical Southern-hemisphere-indicator, using the binary variate $S$ (short for Southern) that takes the value 0 if the Northern hemisphere, and  1 if the Southern hemisphere.
		\pause 
	\item 	But go back to  some long-forgotten mathematics from high school to be able to tell the computer to toggle the ratio off and on. Recall \textbf{powers} of numbers, where, for example, 
		'$y$ to the power 2', or $y^2$ is the square of $y$. The two powers we exploit are 0 and 1.
		'$y$ to the power 1', or $y^1$ is just $y$ and '$y$ to the power 0', or $y^0$ is 1.
		\pause 		
\item 		We take advantage of these to write
		$$\mu_S = \mu \ | \ S  \ = \mu_0 \ \times \  \Big\{ \frac{\mu_{South}}{\mu_{North}}\Big\}^S = \mu_0 \ \times Ratio \ ^ S.$$ 
		
	\item 	You can check that it works for each hemisphere by setting $S=0$ and $S=1$ in turn. Thus, $$\log(y^S) = S \times \log(y)$$  
		
	\end{itemize}
\end{frame}


\begin{frame}[fragile,plain]
	<<eval=T, echo=F, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F>>=
			plot3(4)
	@
\end{frame}



\begin{frame}
\begin{itemize}
	\item Although this is a compact and direct way to express the
	parameter relation, it is not well suited for fitting these equations to data. 
	
\item 	However, in those same  high school mathematics courses, you also learned about \textbf{logarithms}. For example, that 
	$$\log(A \times B) = \log(A) + \log(B); \ \  \log(y^x) = x \times \log(y).$$
	\pause 
	\item Thus, we can rewrite the equation in panel (c) as
	
	$$\log(\mu_S) = \log(\mu \ | \ S)  \ = \underbrace{\log(\mu_0)} \ +  \underbrace{\log(Ratio)} \times \ S.$$
	
	\item This has the same 'linear in the two parameters' form as the one for the parameter difference: the parameters are
	$\underbrace{\log(\mu_0)}$ and $\underbrace{\log(Ratio)}$ and they are made into the following 'linear compound' or 'linear predictor' (see Remarks below) :
	$$\log(\mu_S) = \log(\mu \ | \ S)  \ = \underbrace{\log(\mu_0)} \times \ 1 \ + \underbrace{\log(Ratio)} \times \ S.$$
	\pause 
\item 	The course is concerned with using 'regression' software to  'fit'/'estimate' these 2 parameters from $n$ depth measurements indexed by $S$.
	
\end{itemize}
\end{frame}


\begin{frame}{Parameter Functions}
\begin{itemize}
	\item A very simple example of a function that describes how  parameter values vary over quantitative levels of a determinant is the straight line shown in the upper right panel of the next figure. Here the determinant has the generic name X, and the equation is of the $A + B \times X$ or $B_0 + B_1 \times X$ or $\beta_0 + \beta_1 \times X$ straight line form. 
	
	\item Miettinen used the convention that the upper case letters $A$ and $B$ are used to denote the (true but unknown) coefficient values, whereas the lower case leters $a$ and $b$ are used to denote their empirical counterparts, sometimes called estimated coefficients or fitted coefficients.  This sensible and simple convention also avoids the need, if one uses Greek letters for the theoretical coefficient values, to put 'hats' on them when we refer to their empirical counterparts, or 'estimate/fit' them. Fortunately, journals don't usually allow investigators to use 'beta-hats'; but this means that the investigators have to be more careful with their words and terms.
	

\end{itemize}
\end{frame}


\begin{frame}{Parameter Functions}
	\begin{itemize}
		
		\item As we go left to right in the following grid, the models become more complex. The simplest  is the one of the left, in column 1, the one  JH refers to as 'the mother of all regression models.'  It refers to a \textit{single} or \textit{overall} situation/population/domain, so $X \equiv 1$, it takes on the value 1 in/for every instance/member. 
		
		
		\item So the parameter equation is $\mu_X = \mu \ | \ X \ = \mu \times 1.$ In column 2, there are 2 subdomains, indexed by the 2 values of  the determinant (here generically called 'X'), namely $X = 0$ and $X = 1$. In the 3rd column, the number of of parameters is left unspecified, since the numbers of coefficients to specify a line/curve might vary from as few as 1 (if we were describing how the volume of a cube dependeds or, was is function of, its radius) to 2 (for a straight line that did not go through the origin, or for a symmetric S curve) to \textit{more than 2} (e.g., for a non-symmetric S curve, or a quadratic shape).
	\end{itemize}
\end{frame}


\begin{frame}[fragile,plain]
	<<eval=T, echo=F, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F>>=
	expit = function(x) exp(x)/(1+exp(x)) # antilogit
	
	par(mfrow=c(1,1),mar = rep(0.01,4))
	
	plot(c(-0.75,4),c(1,4.5),col="white",axes=FALSE,frame=FALSE)
	
	
	segments(0.5,1:4,4,1:4,col="blue",lwd=2)
	segments(1:4,1,1:4,4,col="blue",lwd=2)
	
	CEX=1.25
	
	x=0.5
	dy=0.2
	dx=0.25
	for(row in 3:1) {
	if(row==3) {txt = expression(mu);TXT="MEAN"}
	if(row==2) {txt = expression(pi);TXT="PROPORTION"}
	if(row==1) {txt = expression(lambda);TXT="RATE"}
	text(x-0.1,row+.5,txt,cex=2*CEX,adj=c(0,.5))
	text(x-0.25-0.05,row+.5,TXT,adj=c(1,0.5),cex=CEX)
	if(row==1)text(x-0.25-0.05,row+.35,"(Number of events
	per unit time)",
	adj=c(1,1),cex=0.8*CEX)
	arrows(x+dx, row+dy, x+dx,row+1-dy,angle=30,
	length=0.07-0.07*(row==2),code=2+(row==3))
	if(row<3) text(x+dx,row+dy,"0",adj=c(1.5,0.5))
	if(row==2)text(x+dx,row+1-dy,"1",adj=c(1.5,0.5))
	}
	
	dy=0.2
	text( 2+0.5,4.45, "Number of Parameters",cex=1.75,adj=c(0.5,1))
	
	text( 0,4.3, "Parameter\nGenre", cex=1.75,adj=c(0.5,0))
	arrows(0,4.25,0,3.9,length=0.07,angle=35,lwd=2)
	for(col in 1:3) {
	x=col+0.5; 
	text(col+0.5, 4.15, c("1","2","?")[col],
	adj=c(0.5,1),cex=1.5)
	if(col==1) dx=0.05
	if(col==2) dx=0.3
	for(row in 3:1){
	segments(col+0.5-dx,row+dy,col+0.5+dx,row+dy)
	if(col>1)text(col+0.5-dx,row+dy,"0",adj=c(0.5,1.75))
	if(col==2) text(col+0.5+dx,row+dy,"1",adj=c(0.5,1.75))
	if(col==3) {
	xx=seq(col+0.5-dx,col+0.5+dx,dx/10)
	segments(xx,row+dy,xx,row+dy-0.03)       
	}
	txt=expression(X)
	if(col==1) txt = expression(X %==% 1)
	text(col+0.5,row+dy/3,txt,font=2)
	if(col==1) { m= (2/3)*(1-2*dy)
	points(col+0.5,row+dy+m,pch=19,cex=0.6)
	}
	if(col==2) { m= ((1:2)/3)*(1-2*dy)
	points(col+0.5+c(-dx,dx),row+dy+m,pch=19,cex=0.6)
	}
	if(col==3) { m= ((3:12)/14)*(1-2*dy)
	xx=seq(-dx,dx,length.out=10)
	points(col+0.5+xx,row+dy+m,pch=19,cex=0.6-0.3*(row<3))
	if(row==2){
	logit = seq(-3,3,length.out=10)
	m=expit(logit)*(1-2*dy)
	points(col+0.5+xx,row+dy+m,pch=19,cex=0.6)
	}
	if(row==1){
	log.rate = seq(-1.7,0.1,length.out=10)
	m=exp(log.rate)*(1-2*dy)
	points(col+0.5+xx,row+dy+m,pch=19,cex=0.6)
	}
	}
	} 
	}
	
	@
\end{frame}


\end{document}


