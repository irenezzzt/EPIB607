# Random Variables/Variation {#randomVariables}


## Objectives

This **central** chapter addresses a **fundamental concept**, namely the **variance of a random variable**. It gives the laws governing the variance of a sum of 2, or (especially) $n$ random variables  -- and even more importantly -- the laws governing **the variance of a difference of two random variables.** The latter is  central, not just to simple contrasts involving just 2 sample means or proportions, but also in the much wider world of regression, since the variance (sampling variability) of any regression slope can be viewed as the **variance of a linear combination of** random 'errors', or random deviations, or **random variables**.   So, if there is one 'master' formula to pay attention to and to 'own', it is the one for the variance of a linear combination of random variables. All others are special cases of this.

So, the **specific objectives** are to truly understand

* the concept of  a random variable.

* the concept of the (expectation and) variance of a random variable.

* why it is that, when dealing with the sum  of two or more independent random variables, it is not their standard deviations that sum (add), but rather their variances.

* likewise, when dealing with the **difference**  of two independent random variables, or some **linear combination** of $n$ independent random variables involving positive and negative weights, why it is that **the component variances add**, and **with what weights**. 

## Random Variables

**Textbook definitions of RANDOM VARIABLE**

(Note: JH has changed the usual $X$ to $Y$) 

> A variable ($Y$) whose value is a number determined by the outcome of an experiment

> A variable ($Y$) whose value is a numerical outcome of a random phenomenon
 
 
**Textbook definitions: DISCRETE Random Variable**
 
> A random variable that assumes only a finite (or countably infinite) number of distinct values
 
> Discrete random variables have "a finite or countably infinite number of possible values, each with positive or zero probability.

> A discrete random variable $Y$ has a finite number of possible values
  
  
Random Variable: DISCRETE or CONTINUOUS ?

* _How long_ you have to wait for bus / elevator / surgery/ download to complete
* the _blood group_ of n = 1 randomly selected person
* _how many tries_ before pass a course
* _how many_ of n = 20 randomly selected persons will return questionnaire in pilot study
* _length of song_ on a CD
* _mean cholesterol level_ in sample of n = 30 randomly selected persons
* _how hot_ it is going to be today
* _how much_ snow we will get next winter
* _time_ someone called (on answering machine)
* _value of test-statistic_ if 2 populations sampled from have the same mean
* _how much_ ice McDonalds puts in soft drink 
* _how many_ calories in hamburger 
* _how many_ numbers you get correct in 6/49? 
* _where_ roulette wheel stops
* _how many_ "wrong number" calls received
* _how many_ keys you have to try before get the right one 
* _how much_ water consumed by 100 homes

**PROBABILITY DISTRIBUTION $p(y)$ associated with (Discrete) Random Variable $Y$**

The ordered pairs {y, Probability($Y$ = $y$) } , where $y$ ranges over the possible values of $Y$

* Probability($Y$ = $y$) is often shortened to Prob($Y$ = $y$) or P($Y$ = $y$)

* Can display distribution as formula, table, graph, etc.

EXAMPLES

(Note: the only reason we start with lotteries is that we can give the exact, mathematically-known, probabilities of each possible result. In most of the other examples, the probabilities will be context-specific, and seldom knowable with full exactness.)

* Y = Winnings on a $1 wager on la Quotidienne 3
(same distribution for exact order and any order)
 
`....... ($)`  
`........  y .. Probability(Y = y)`

`........ 0 ............... 0.999`  
`...... 450 ............... 0.001`  
`.......................... =====`  
`...... AL................. 1.000`  

* Randomly select n = 1  person  
Y = the person's Blood Group

`.... y   Probability(Y = y)`

`.... A .............. p(A)`  
`.... B .............. p(B)`  
`.... AB ............ p(AB)`  
`.... O .............. p(O)`  
`.................... =====`  
`....ALL ............ 1.000`  

* In a 1911 census, of all households with at least 1 child aged 12 or under, the proportions of households that had 1, 2, ... children. [We ignore for now any errors in recording ages]


```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

fr = c(15273, 11193, 8226, 5761, 3300, 1397, 404, 97)

proportion.of.households =round(fr/sum(fr),3)
n.children = 1:8
cbind(n.children,proportion.of.households)

```


* Proportions of (currently) married women aged 45 and older who, in a 1911 census, reported this "number of children born alive to the present marriage"


```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

f=c(4780, 935, 1142, 1331, 1425, 1529, 1522, 1453, 1416, 1214, 1085, 667, 511, 275, 156, 85, 36, 20, 11, 6, 1)
proportion.of.women =f/sum(f)
proportion.of.women = round(proportion.of.women,6)
n.children = 0:20
cbind(n.children,proportion.of.women)

```

* Choose a random word and count how many characters (letters) it contains  
Y =  Number of characters in word

`.... y . Probability(Y = y)`

`.... 1 .............. p(1)`  
`.... 2 .............. p(2)`  
`.... etc. ........... etc.`  
`.................... =====`  
`... ALL ............ 1.000`  


* Choose a random hospital admission and count how many days it lasted

Y =  Length of Stay (LOS)

`.... y ..... Probability(Y = y)`

`.... 1 day ............... p(1)`  
`.... 2 days .............. p(2)`  
`.... etc. ................ etc.`  
`......................... =====`  
`.... ALL ................ 1.000`  


Note 1: The probabilities (or if you prefer to think of them as proportions) add to 1. We use probabilities or fractions as relative frequencies (like a histogram with an infinite number of entries)

Note 2: typically, the random quantity is obtained from an aggregate of elements e.g. a sum, mean , proportion, regression slope

MORE EXAMPLES OF PROBABILITY DISTRIBUTIONS

(Some are on a continuous scale, but for convenience, the possible values are shown 'binned' into intervals)

* To pilot-test what the return rate of a mail survey is going to be, you mail the questionnaire to n = 20 randomly selected persons.  
Y = the number who will return questionnaire

`...... y  Probability(Y = y)`  
`...... 0 ............. p(0)`  
`...... 1 ............. p(1)`  
`...... 2 ............. p(1)`  
`....... .............. ....`  
`..... 19 ............ p(19)`  
`..... 20 ............ p(20)`  
`..................... =====`  
`..... ALL ........... 1.000`  


* People call out their birthdays:  
Y = when get 1st duplicate

`...... y  Probability(Y = y)`

`...... 2 ............. p(2)`  
`...... 3 ............. p(3)`  
`....... ...................`  
`.... 366 ........... p(366)`  
`.................... =====`  
`..... ALL .......... 1.000`  

* Y = Winnings on a $1 wager on la Quotidienne 4
(same distribution for exact order and any order)
 
`....... ($)`  
`........ y .. Probability(Y = y)`  

`........ 0 ............... 0.999`  
`..... 4500 ............... 0.001`  
`.......................... =====`  
`..... ALL ................ 1.000`  


* Y.bar = Mean cholesterol level in randomly selected sample of n = 30 persons

`.... y.bar .. Probability(y.bar would be...)`

`.. <  4.5 mmol/L ............. p( . )`  
`.... 4.5 - 4.6 .............. p( . )`  
`.... 4.6 - 4.7 .............. p( . )`  
`.... 4.7 - 4.8 .............. p( . )`  
`.... 4.9 - 4.9 .............. p( . )`  
`.... 4.9 - 5.0 .............. p( . )`  
`.... 5.0 - 5.1 .............. p( . )`    
`.... 5.1 - 5.2 .............. p( . )`  
`.... etc`  
`.... 5.6 - 5.7 .............. p( . )`  
`.... 5.7 - 5.8 .............. p( . )`  
`.... 5.8 - 5.9 .............. p( . )`  
`.... 5.9 - 6.0 .............. p( . )`  
`........ > 6.0 .............. p( . )`  
`............................. =====`  
`..... ALL ................... 1.000`  


* Y = the value of the test- statistic if 2 populations sampled from had the same population mean

`.... test.statistic (Z) .. Probability(Z would be...)`

`.. < -2 ........................... 0.028`  
`.... -2 to -1 ..................... 0.136`  
`.... -1 to 0 ...................... 0.341`  
`..... 0 to +1 ..................... 0.341`  
`.... +1 to +2 ..................... 0.341`  
`.........> +2 ..................... 0.028`  
`................................... =====`  
`..... ALL ......................... 1.000`  


* In 6/49 lottery, player selects 6 distinct numbers on a grid showing the numbers 1 to 49.  
49 otherwise identical balls, but numbered 1 to 49, are thoroughly mixed in an urn  
6 balls are drawn without replacement.  
.  
Y = how many of the balls drawn show numbers that match the numbers selected by player


`......... y .. Probability(Y = y)`

`......... 0 .......... 0.4359650`  
`......... 1 .......... 0.4130195`  
`......... 2 .......... 0.1323780`  
`......... 3 .......... 0.0176504`  
`......... 4 .......... 0.0009686`  
`......... 5 .......... 0.0000184`  
`......... 6 .......... 0.0000001`  
`...................... =========`  
`....... ALL .......... 1.0000000`  

> http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/banco.pdf


> http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/Keno(20-spot).pdf

## Expectation (mean)  of a Random Variable

**DEFINITION**

If $Y$ takes on the DISCRETE values

... $y_1$ with probability $p_1$,  
... $y_2$ with probability $p_2$,  
... $y_3$ with probability $p_3$,  
... etc
... $y_k$ with probability $p_k$,  

then the **expected value of $Y$ (written 'E($Y$)' )** is

$$E[Y] = y_1 \times p_1 + y_2 \times p_2 + \dots + y_k \times p_k = \sum y_i \times p_i.$$  

E($Y$) is a mean that uses expected (i.e. unobservable or theoretical or long run) relative frequencies ($p$'s). ( $\bar{y}$ uses observed relative frequencies. )

Can think of E($Y$) as 'center of mass' of $p().$

If $Y$ takes on the CONTINUOUS values $y$ - $\delta y/2$ to $y$ - $\delta y/2$ with probability
$p = f(y) \delta y$, then
$$ E[Y] = \int f(y) dy.$$

**RELEVANCE** of Expectation of a Random Variable

* IT ACTS AS A MEAN FOR A VARIABLE THAT HAS A (CONCEPTUAL) REPETITION OR AN INFINITE N

* THE EXPECTED VALUE OF A RANDOM VARIABLE Y WILL USUALLY BE IN TERMS OF POPULATION PARAMETERS.  
.  
A STATISTIC WITH EXPECTED VALUE $\theta$ IS AN 'UNBIASED ESTIMATOR' OF $\theta$.

   + example 1:  
Y = Proportion of 'YES' in sample on $n$  
E($Y$) = $\pi$ = PROPORTION of YES' in POPULATION  
Then $\hat{\pi}$ = $Y,$  and Y is an unbiased estimator of $\pi.$

   + example 2:  
If we use a divisor of $n - 1$ to calculate the sample variance $s^2 = \frac{\sum(y-\bar{y})^2}{n-1},$ then  
E($s^2) = \sigma^2$, so
$s^2$ is an unbiased estimator of $\sigma^2$. ( $\widehat{\sigma^2}$ stands for 'estimate/estimator of' $\sigma^2.$)  
If we use a divisor of $n$, then  
$$E\bigg(\frac{\sum(y-\bar{y})^2}{n}\bigg) = \frac{n-1}{n} \times \sigma^2.$$  
This estimator produces estiamtes that, on average, are too small.

**EXAMPLES** of EXPECTED VALUE of Random Variable

* Y = Winnings on a $1 wager on la Quotidienne 3

 
`..... y($) ... p(y) ...... y x p(y)`

`........ 0 .. 0.999 ....... $0.00`  
`...... 450 ...0.001 ....... $0.45`  
`.......................... =====`  
`...... ..............SUM .. $0.45`  

Some may find it easier to think of averaging the winnings of 1 person who won 450 dollars and 999 persons who won 0 dollars.

* Keno: Y = Winnings on a $3 wager  
.  
E(Winnings) = $2.12 (> 70%) [see here.]( http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/Keno(20-spot).pdf) 
  
*  Banco: Winnings on a $1 wager  
.  
42% ≤ E(Winnings) ≤ 55%, depending on how many numbers played [see here.](http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/banco.pdf)

* **Longevity of a fictitious  birth cohort** if they were to experience age-specific death rates observed in Quebec in 1990 [see exercises in 3rd computing session]

   + Y = Length of life = age at death.  
Y = Age at death (or Longevity or length of Life, or lifetime, if you prefer to be positive)  .
.
E(Y) = E(Lifetime) = '**LIFE EXPECTANCY AT BIRTH**'  
.  
Assume for sake of illustration that deaths in a decade are all at midpoint of interval (calculations done using one-year-wide age-bins rather than one-decade-wide would be more exact)

`age = mid-point of decade (for simplicity)`  
` p = proportion dying in decade `  
`....................MALES .............. FEMALES`  

`decade .... age ..... p .... age x p ...... p ...... age x p `  
.  
`00-10 ...... 05 .... 0.010 ... 0.050 ..... 0.080 ..... 0.040`  
`10-20 ...... 15 .... 0.006 ... 0.089 ..... 0.002 ..... 0.030  `  
`20-30 ...... 25 .... 0.012 ... 0.295 ..... 0.004 ......y.yyy `  
`30-40 ...... 35 .... 0.016 ... 0.544 ..... 0.007 ......y.yyy `  
`40-50 ...... 45 .... 0.030 ... 1.335 ..... 0.017 ......y.yyy `  
`50-60 ...... 55 .... 0.074 ... 4.079 ..... 0.040 ......y.yyy `  
`60-70 ...... 65 .... 0.180 .. 11.697 ..... 0.096 ......y.yyy `  
`70-80 ...... 75 .... 0.301 .. 22.610 ..... 0.214 ......y.yyy `  
`80-90 .. ... 85 .... 0.279 .. 23.680 ..... 0.358 ......y.yyy `  
`90-100 ..... 95 .... 0.093 ... 8.822 ..... 0.254 .....24.136 `  
`.................... =.=== .. ==.=== ..... =.=== .... ==.=== `  
`.ALL ............... 1.000 .. 73.2 ....... 1.000 .... xx.x`


* Mean no. of children 12 years and under in households with at least 1 such child:

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}



fr = c(15273, 11193, 8226, 5761, 3300, 1397, 404, 97)
proportion.of.households =round(fr/sum(fr),3)
n.children = 1:8
product = n.children * proportion.of.households
cbind(n.children,proportion.of.households, product)
cat(paste(". Sum(products) = Expected Value = Mean = ",round(sum(product),1)))

```

* If you want to start **your own Insurance Company** 
Y = Payout (from -99,750 to +$1250)  

E(Payout on single policy) > 0 BUT...
Variance(Payout) is VERY LARGE  


see MM3 p 341

* Distance Where to wait if 3 unequally spaced elevators ?
prob(it's #1) = p(it's #2)=p(it's #3) = 1/3


## Expected value  of a FUNCTION of a  random variable

* If $Y \ \sim  \ p(y)$, i.e., $Y$ has probability distribution p($y$), and if $g(Y)$ is some real-valued function of $Y$, then
$$E[ g(Y) ] = \sum g(y)  \times p(y),$$
i.e. it is a weighted mean of the $g(y)$'s, with $p(y)$'s as weights.

**NOTE** In some instances, the expectation of $g(Y)$ is $g(E[Y])$, while in others it is more complex. Can you figure out when it is/is not in each of the following instances?

* **Examples** 

   + $Y$ = Noon Temperature (C) in Montreal on a randomly selected day of the year;  
$g(Y)$ = Temperature (F) = 32 + (9/5) $Y$

   + $Y$ = Weight in Kg (or Height in cm) of a randomly selected person;  
g(Y) = Weight in Kg (or Height in inches)

   + $Y_1$ and $Y_2$ are two random variables that might or might not be related;  
.  
if $g(Y_1, Y_2) = Y_1 + Y_2,$ then  $E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].$  
.  
if $g(Y_1, Y_2) = \frac{Y_1 + Y_2}{2},$  then $E[g(Y_1, Y_2)] = \frac{E[Y_1] + E[Y_2]}{2},$  
.  
and, by analogy, for a sum or mean of $n$ related or unrelated random variables.

   + $Y$ = diameter of a randomly chosen sphere;  
$g(Y)$ = Volume of sphere = $\frac{\pi}{6}  Y^3.$

   + $Y$ = fuel consumption, in liters/100km, of  a randomly selected make of car;  
$g(Y)$ =  miles per gallon or Km per liter (reciprocal)

   + $Y$ =  which of 3 [**unequally spaced elevators**](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/median-elevator.pdf) shows up next.  
Prob(it is #1) = prop(it is #2) = prob(it's #3) = 1/3.  
.  
$g(Y)$ = Distance to elevator. How to mimimize E(distance)?  
.  
$g(Y)$ = **Squared Distance** to elevator. How to mimimize E($g(Y)$)?  


   + Random Variable Y with Expectation or Mean $\mu$  
.  
$g(Y) = (Y - \mu)^2$, the **squared deviation from the mean**

**The following is in bold to emphasize one of the most fundamental concepts in statistics, namely VARIANCE.**

## Variance (and thus, SD) of a random variable

### Definitions

* E[$(Y - \mu)^2$] is called the VARIANCE of the random variable $Y$. It is usually shortened to Var($Y$) or even to V($Y$).

* **It, and its positive square root, called the _standard deviation_ of $Y$, or SD($Y$), are two of the most commonly used measures of variability or spread or uncertainty.**

*  Computationally, Variance($Y$) = E[ $(Y - \mu)^2$ ] = $\sum(y - \mu)^2 \times f(y),$ or **Mean Squared Deviation**, and

* Standard Deviation, SD($Y) = \sqrt{(Var(Y)}= \sqrt{E[ (Y — \mu)^2]},$ or **Root Mean Squared Deviation**

* In **French**, the Standard Deviation is called [**écart type**](https://fr.wikipedia.org/wiki/Écart_type).  
This [French->English dictionary](http://www.french-linguistics.co.uk/dictionary/type.html) translates (the noun) `écart` as space, gap, distance between objects, interval, gap between dates, difference between numbers, opinions; à l'~ isolated, remote, out-of-the-way; à l'~ de well away from; ~ de conduite misdemeanour; ~ d'inflation inflation differential; ~ de langage strong language, bad language; ~ type standard deviation and  
(the adjective) `type` translates as typical, standard; lettre ~ standard letter; (Maths) écart ~ standard deviation. JH thinks this adjective better describes the meaning that 'standard' does. See here for the **history of the term** [`standard` deviation](https://en.wikipedia.org/wiki/Standard_deviation#History). 


Here, graphically and numerically illustrated, are three (of the many) ways to measure the variability of a random variable.

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="6 symmetrically distributed random variables, and 3  ways of measuring their spreads about a common mean. [After Mosteller, Rourke and Thomas. Probability with statistical applications 2nd Ed, p205] "}

Y.values = list(y1 = (-1):1,
                y2 = (-1):1,
                y3 = c(-1,1),
                y4 = (-2):2,
                y5 = c(-2,2),
                y6 = c(-3,-2, 2,3))

Probs = list(p1 = c(1,6,1),
             p2 = rep(1,3),
             p3 = rep(1,2),
             p4 = rep(1,5),
             p5 = rep(1,2),
             p6 = rep(1,4) )

par(mfrow=c(1,1),mar = rep(0,4))

plot(1, ylim=c(0.5,7.45),xlim=c(-3.75,11.25),
     col="white", frame=FALSE)

measure = c("Mean\nAbsolute\nDeviation",
            "Mean\nSquared\nDeviation\n(Variance)",
            "Root\nMean\nSquared\nDeviation\n(SD; \u{E9}cart type)")
for(i.r in 1:6) {
  segments(-3,i.r, 3,i.r)
  v = Y.values[[i.r]]
  h = Probs[[i.r]]
  h.sum = sum(h)
  pr = h/h.sum 
  for (i in 1:length(v)){
     text(v[i], 7-i.r, toString(v[i]), adj=c(0.5,1.25))
     segments(v[i],7-i.r,v[i], 7-i.r + pr[i], lwd=2.5  )
     text(v[i], 7-i.r + h[i]/h.sum, 
         paste(toString(h[i]),"/", toString(h.sum) ),
         adj = c(1.1,0),font=3)
     for(j in 1:3){
       if(j==1) spread = sum(abs(v)*pr) 
       if(j==2) spread = sum((v^2)*pr)
       if(j==3) spread = sqrt( sum((v^2)*pr)  )
       text(2.5 + 2.5*j,7-i.r, toString(round(spread,2)))
       if(i==1) text(2.5+2.5*j, 6.85, measure[j], cex=1 )
     }
  }

}

```

In practice, the mean absolute deviation is often quite close to the SD, and certainly easier to explain to explain to non-statisticians. However, when computig was by ahnd and laborious, it took two passes through the data to compute it, whereas, the SD could be computed in one.  

* **Variance & SD of number of children <= 12 years** in households with at least 1 such child:

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

fr = c(15273, 11193, 8226, 5761, 3300, 1397, 404, 97)
proportion.of.households =round(fr/sum(fr),3)
n.children = 1:8

Mean = sum(product)
devn. = n.children - Mean
devn.squared = devn.^2
product = devn.squared * proportion.of.households
cbind(n.children, devn.,devn.squared,proportion.of.households, product)
cat(paste("....... Sum(products) = Mean of Squared Deviations = Variance =",round(sum(product),2)))

cat(paste(". Sq. Root of Mean of Squared Deviations = Standard Deviation =",round(sqrt(sum(product)),1)))

```




**Which is primary, Standard Deviation or Variance?**

* Although we first define variance and then take the square root to reach the SD, we should think of the SD as primary, at least for descriptive purposes (Mosteller et al. use the natural order "...standard deviation and variance (either of these measures determines the other because the variance is the square of the SD"...  However, there are _good mathematical reasons_ to work with variance.

### Some (good) reasons for using variance, which averages the squares of the deviations from the mean.

   + ADDITIVITY  
The variance of the sum of two independent random variables is the sum of their variances, and even when the two variables are dependent the variability of their sum has a simple formula. SD;s dont add; their squares do. Or to quote the physicists, errors '**Errors add in quadrature**, like the lengths of the sides of Pythogoras' traingle. It took mathematicains a long time to discover, this, and some of the blunders along the way are told in a fascinating chapter in this very readable book [The Seven Pillars of 
Statistical Wisdom](https://www-degruyter-com.proxy3.library.mcgill.ca/view/title/521193).  

   + THE CENTRAL LIMIT THEOREM  
The limiting behavior of a random variable that is the sum of a large number of independent random variables depends on the variances of these random variables.


* USEFUL RELATIONSHIP/SHORTCUT (especially for hand computation, 'back when', and in mathematical statistics, still today):  
$$Variance(Y) = E(Y^2) - \mu^2.$$
$$\textrm{Variance =  average  square  minus   squared average}.$$




### But, for  end-users today ....

Jerry Hill, who wrote 
[this PhD thesis](https://www.collectionscanada.gc.ca/obj/s4/f2/dsk2/ftp02/NQ26855.pdf) 'at the end of a career in medicine and epidemiology' commuted from Ottawa and taught the 607 course a few times in the 1980s. In the course, he used to joke about  the (many) mathematical statisticians who refer to a random variable having a  Normal (Gaussian) distribution with standard deviation $\sigma$ as $Y \sim N(\mu, \sigma^2)$, i.e., who **defined the spread of the distribution (any distribution, not just Gaussian ones) using its variance**. [Note that the `dnorm`, `pnorm` `qnorm` and `rnorm` functions in `R` all use the argument `sd`, as do all applied textbooks, writing $Y \sim N(\mu, \sigma)$]. Gerry would then go on to give a numerical example, using the  commonly used metric the Total Fertility Rate (TFR) – or often simply 'fertility rate' – which (as per Goggle) `measures the **average number of children per woman**.' The global average fertility rate is just below **2.5 children per woman** today'. He would them say, 'Suppose the variation from country to country had a **standard deviation of 1.2**'. Then, reminding his class to use appropriate units when reporting statistical measures, he would ask that they give the appropriate units for the 2.5, the 1.2, and the $1.2^2$ = 1.44, forcing them to say the **variance was 1.44 square children per square woman**. 

### Example of Variance-calculation using one-pass formula

**Number of children born alive**

Proportions of (currently) married women aged 45 and older who, in a 1911 census, reported this "number of children born alive to the present marriage"


```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

f=c(4780, 935, 1142, 1331, 1425, 1529, 1522, 1453, 1416, 1214, 1085, 667, 511, 275, 156, 85, 36, 20, 11, 6, 1)
proportion.of.women =f/sum(f)
proportion.of.women = round(proportion.of.women,6)
n.children = 0:20
Mean = sum( n.children * proportion.of.women )
squared.children = n.children^2
product = squared.children * proportion.of.women
cbind(n.children,squared.children,
      proportion.of.women,product)[ 1:3,]
cbind(n.children,squared.children,
      proportion.of.women,product)[ 19:21,]

cat(paste("Sum(product) = Mean Squared Value = ", round(sum(product),2), "; Mean Value = ",round(Mean,2)))

cat(paste("Variance = ",
          round(sum(product),2),
          " minus the square of" ,
          round(Mean,2),
          " = ",
          round(sum(product)-Mean^2,2)))

cat(paste("Thus, Standard Deviation = sqrt(",
          round(sum(product)-Mean^2,2),
          ") = ",
          round(sqrt( sum(product)-Mean^2 ),2)
          ))

```
 
## Variance and SD of a FUNCTION of a random variable

If we go back to some of the  **examples listed above** we can reason out what the law must be

   + $Y$ = Noon Temperature (C) in Mtl on a randomly selected day of the year;  
... $g(Y)$ = Temperature (F) = 32 + (9/5) $Y$  
If the SD was say 10 C, then surely the SD is 18 C. After all, Temperature is Temperature, so you are not -- as some textbooks are inclined to say -- 'changing' the fundamental variable, but rather changing the scale of the temperature variable. So, the SD scales up by 9/5, and so, being **an average square**, the **variance scales up by $(9/5)^2$.** If you are going the other way, from the larger F scale to the smaller C scale, the scalings are (5/9) for the SD, and $(5/9)^2$ for the variance. More generally,  
$$SD[constant  \times RV] = constant  \times SD[RV]$$  
$$Var[constant  \times RV] = constant^2  \times Var[RV]$$
.  
This example also shows another law related to spread: shifting left or by a constant amount (eg. suppose the conversion was $F = 10 + (9/5)C$ instead of was $F = 32 + (9/5)C,$ it would not alter the spread. Thus,  
.  
$$SD[ RV + constant ] = SD[RV]$$  
$$Var[ RV + constant ] = Var[RV]$$

   + $Y$ = Weight in Kg (or Height in cm) of a randomly selected person;  
... g(Y) = Weight in Kg (or Height in inches)  
.  
This involves just a scaling, with no shift. So if the SD were 10 Kg, it would be 22 lbs, and the variances in the 2 scales would be $100 \ Kg^2$ and $484 \ lb^2$.

   + $Y$ = **Years of publication** of all the books in the McGill Library, with Years measured from AD (Anno Dominini, 'in the year of the Lord').  
.  
The SD would be the same if we measured the Year from 1439 AD ($Y' = Y$ - 1439) when Gutenberg  was the first European to use movable type, or from 1492 AD when Christopher Columbus reached North America.  
.  
What if, instead, we calculated the **age of each book in the year 2020**, i.e. as $Y' = 2020 - Y$?  
.  
The **scale would now be reversed**. Instead of being at the extreme left, the older books would not be at the right hand of the scale, and vice versa. But the spread would still be the same, even though the shape of the new distribution would be the miorrow inage of the old one.  
.  
What if we measured **age in decades**, i.e., $Y' = \frac{2020 - Y}{10}$ or centuries i.e., $Y' = \frac{2020 - Y}{100}$?  
.  
The SDs would be scaled down by 10 and by 100, and the variances by $10^2$ and $100^2.$

   + $Y$ = Ocean Depth ot a randomly chosen location, measured in metres. If the **'origin' is the ocean floor**, all depths will be positive; if it is **the surface of the ocean**, they will all be negative. The **spread, the SD and the Variance stay the same**, but the **shapes of the distributions are mirror inages** of each other. 


   + $Y$ = diameter of a randomly chosen sphere;  
... $g(Y)$ = Volume of sphere = $\frac{\pi}{6}  Y^3.$  
.  
This one is more complicated -- as you might have guessed from just trying to compute the expectation. The fact that the scaling is different at different values of $Y$ complicated matters. There is an appriximate formula, that depends on the scaling at some 'representative' value of $Y$, typically the mean or mode. For more on this, see the examples in this [expository piece](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/jh_dt_tas_2006.pdf).  

   + $Y$ = fuel consumption, in liters/100km, of  a randomly selected make of car;  
$g(Y)$ =  miles per gallon or Km per liter (reciprocal)  
.  
There is no exact closed form, but the approximation works well because the values are well away from zero, and not too spread out -- unless you allow Hummers!

## Sums/means/differences of RVs

### A sum (of 2 or $n$)

To keep it simple, and to allow us to see what is going on, Let's consider two very simple random variables (RV's), each taking just 2 values, and with equal probabilities. [You can check for yourself later that the same law applies to random variables that take on more than 2 values, and with uneven probability distributions.] The key is that the 2 variables are _independent_ of each other.

The next panels show the two RV's.  $RV_1$ \ (6 \ or \ 12) \ and \ $RV_2$ \ (8 \ or \ 16)

$$Sum = RV_1 \ (6 \ or \ 12) \ + \ RV_2 \ (8 \ or \ 16) \ = \ 14 \ or \ 20 \ or \ 22 \ or \ 28.$$  
We will come back much later to the choices of the specific values for each of the random variables; for our purposes here, the main point is that both equally-likely values are an even number apart, so the SDs are integers, and all calculations involve integers. Note that an RV with equally likely values 1 and 7 (or 3 and 9, or -1 and + 5) has the same SD as the RV with equally likely values 6 and 12: it is the _distance between them_ that determines the SD. Note also, that with half of the values at one extreme and hald at teh other, all values are exactly one SD from the mean. [If you want to give them some meaning so that you can relate to them, thing of them as the durations of 2 stages in an industrial process, or a service (such as processing online orders, and arrranging shipping and delivery, or waiting for a mammogram, and then a biopsy).]  

```{r,eval=T, echo=F, fig.align="center", fig.height=3, fig.width=9, warning=FALSE, message=F}

plot.rv = function(v,COL,LAB){
   CEX=1.1
   par(mfrow=c(1,1),mar = rep(0,4))
   plot(-10,-10,xlim=c(-5,40),ylim=c(0.375,0.57),
     xaxt="n",yaxt="n",frame=TRUE)
   text(v[2]+2,0.52,LAB,cex=1.5,adj=c(0,0))
   arrows(mean(v),0.45,v,0.45,length=0.06,angle=35,col=COL)
   points(v,rep(0.505,2),pch=19,col=COL)
   points(mean(v),0.45,pch=19,cex=CEX/2,col=COL)
   text(3,0.55, "PROBABILITY:",cex=1.1*CEX,adj=c(1,0.5),col=COL)
   for(i in v){
      text(i,0.55,bquote(frac(1, 2)),adj=c(0.5,0.5),col=COL)
      txt = toString( i-mean(v))
      if(i> mean(v)) txt = paste("+",toString( i-mean(v)),sep="") 
      text(i,0.44,txt,adj=c(0.5,1),cex=1.25*CEX,col=COL)
      txt = bquote(frac(1, 2) ~ (-3)^2)
      if(v[1]==8) txt = bquote(frac(1, 2) ~ (-4)^2)
      if(i==v[2]) txt = bquote(frac(1, 2) ~ (+3)^2)
      if(i==v[2] & v[1]==8) txt = bquote(frac(1, 2) ~ (+4)^2)
      text(i,0.40, txt,cex=1.25*CEX,col=COL)
   }
   text( 3,0.40, "VARIANCE  = ",cex=1.1*CEX,adj=c(1,0.5),col=COL)
   text( mean(v),0.40, "+",cex=1.1*CEX,col=COL)
   txt = bquote("= " ~ 3^2)
   if(v[1]==8) txt = bquote("= " ~ 4^2)
   text(v[2]+6,0.40, txt,cex=1.4*CEX,col=COL)

   for(i in seq(0,28,1)){
        segments(i,0.495,i,0.50,col=COL)
        text(i,0.492,toString(i),cex=CEX/1.5,adj=c(0.5,1),col=COL)
   }
   
} 

```


```{r,eval=T, echo=F, fig.align="center", fig.height=3, fig.width=9, warning=FALSE, message=F,fig.cap="Variance of RV1"}

plot.rv(c(6,12),1,expression(RV[1]))

```

```{r,eval=T, echo=F, fig.align="center", fig.height=3, fig.width=9, warning=FALSE, message=F,fig.cap="Variance of RV2"}

plot.rv(c(8,16),1,expression(RV[2]))


```

We now imagine taking a random value of $RV_1$ and  a random value of $RV_2$ and summing them. There are 4 possible sums, and in this case they are all distinct (this isn'y always   be the case, but the values of the two RV's in _this example_ were deliberately chosen to make them all distict, and to avoid grouping RV combinations with the same sum.) A probability tree (next panel) helps to see the 4 possibilities, but here we add an extra feature: we let the lengths of the branches denote the values of the RVs. (This 'stacking' of RV's can have practical advantages too: when Francis Galton wanted to quickly and precisely compute the mean diameter of a large sample of peas, he lined them up in a long groove, and just measured the total width of the entire sample, then divided it by the $n$.)

In our example, the 4 equally likely sums are 14, 20, 22 and 28, and their mean is $\frac{14+20+22+28}{4} = 21.$ That the mean (expected value) of the sum equals the sum of the 2 individual means or expected values, is hardly surprising [ -- and it is even true if the 2 RV's were not independent.] The 4 equally likely deviations from this 21 are -7, -1, +1, and +7, and so, **from 1st principles**, the **Variance of the sum of the 2 RVs is** 
$$\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \frac{100}{4} = 25.$$  
Thus, the **SD of the sum of the 2 RVs is** $\sqrt{25}$ = 5.

Fortunately, we don't have to go back to 1st principles **to calculate the SD of the sum of 2 RVs -- but we do NOT do so by adding the 2 SDs**. IT IS THE VARIANCES THAT ADD! 

```{r,eval=T, echo=F, fig.align="center", fig.height=4.5, fig.width=9, warning=FALSE, message=F}

sum.diff.2rvs = function(v){
   
   CEX=1.1
   
   XLIM=c(-1,42)
   is.SUM = mean(v[,2]) > 0 
   if(!is.SUM) XLIM= c(-26, 14) 
   
   par(mfrow=c(1,1),mar = rep(0,4))
   plot(-10,-10,xlim=XLIM,ylim=c(-0.10,1),
     xaxt="n",yaxt="n",frame=TRUE)
   
   DY = c(0.45, 0.25)
   
   y0 = -0.05
 
   SEQ = seq(0,28,1)
   if(!is.SUM) SEQ = seq(-10,12,1)
   for(i in SEQ){
        segments(i,y0,i,y0+0.01)
        if((i %% 2)==0) text(i,y0-0.01,toString(i),
             cex=CEX/1.5,adj=c(0.5,1))
   }
   
   for (rv1.v in 1:2){
   	   MEAN = mean(v[,1])
   	   y = 0.5 + DY[1] * (rv1.v - 1.5)
   	   txt = bquote(frac(1, 2))
         text(-0.5,y, txt,cex=CEX/1.5, 
              adj=c(1,0.5))
   	   arrows(0, y , 0.98*v[rv1.v,1], y, 
   	          length=0.06, angle=30)
   	   points(v[rv1.v,1], y,  pch=19,cex=0.5 )
   	   
   	   text(  v[rv1.v,1]+1, y, "1")
   	   points(v[rv1.v,1]+1, y, pch=1,cex=2.5 )
   	   
   	   txt1 = toString(v[rv1.v,1] - MEAN)
   	   if(rv1.v==2) txt1 = paste("+",txt1,sep="")
   	   text(v[rv1.v,1], y, txt1,adj=c(0.5,1.35))
   	   Mean = mean(v[,2])
   	   for (rv2.v in 1:2){
   	      yy = y + DY[2] * (rv2.v - 1.5)
   	      txt = bquote(frac(1, 2))
            text(v[rv1.v,1]-0.5*is.SUM +
                            0.5*(1-is.SUM) ,yy,
                 txt,cex=CEX/1.5, 
                 adj=c(!is.SUM,0.5))
   	      arrows(v[rv1.v,1],               yy , 
   	        0.97*(v[rv1.v,1]  + v[rv2.v,2]), yy,
   	        length=0.06, angle=30,
   	        lwd=2)
   	      points(v[rv1.v,1]  + v[rv2.v,2], yy, 
   	             pch=19,cex=0.9 )
   	      
   	      xxc = v[rv1.v,1] + v[rv2.v,2] +
   	               1.5*is.SUM  -1.5*(1-is.SUM)
   	      text(  xxc, yy, "2")
   	      points(xxc, yy, pch=1,cex=2.5 )
   	   
   	   
   	      points(v[rv1.v,1]  + v[rv2.v,2], y0+0.065, 
   	             pch=19,cex=0.9 )
   	      diff = v[rv2.v,2] - Mean
   	      txt2 = toString(diff)
   	      if(diff > 0 ) txt2 = paste("+",txt2,sep="")
   	      
   	   text(v[rv1.v,1]  +v[rv2.v,2], yy-0.035, 
   	        paste(txt1,txt2,sep=" "),
   	        adj=c(0.5,0.5))
   	   
   	   txt = bquote(frac(1, 4))
         text(v[rv1.v,1]  + v[rv2.v,2],yy, 
              txt,cex=CEX/1.5, adj=c(1.05,-0.25))
        
       if(is.SUM){
           if(rv1.v==1 & rv2.v ==1) Txt = 
            bquote((-3)^2 + (-4)^2 + 2(-3)(-4) )
           if(rv1.v==1 & rv2.v ==2) Txt = 
            bquote((-3)^2 + (+4)^2 + 2(-3)(+4) )
           if(rv1.v==2 & rv2.v ==1) Txt = 
            bquote((+3)^2 + (-4)^2 + 2(+3)(-4) )
           if(rv1.v==2 & rv2.v ==2) Txt = 
            bquote((+3)^2 + (+4)^2 + 2(+3)(+4) )
       } 
         
      if(!is.SUM){
           if(rv1.v==1 & rv2.v ==1) Txt = 
            bquote((-3)^2 + (+4)^2 + 2(-3)(+4) )
           if(rv1.v==1 & rv2.v ==2) Txt = 
            bquote((-3)^2 + (-4)^2 + 2(-3)(-4) )
           if(rv1.v==2 & rv2.v ==1) Txt = 
            bquote((+3)^2 + (+4)^2 + 2(+3)(+4) )
           if(rv1.v==2 & rv2.v ==2) Txt = 
            bquote((+3)^2 + (-4)^2 + 2(+3)(-4) )
      }
         
              
       text(41*is.SUM - 26*(1-is.SUM), 
            yy-0.035, Txt,
            adj=c(is.SUM,0.5))

       } #rv2 
   } # rv2
   
   text(41*is.SUM -26*(1-is.SUM),1,
        "The Squared Deviations",
        adj=c(is.SUM,1))
   arrows(0 + 13*(1-is.SUM), y0+0.065, 
         13*is.SUM +6*(1-is.SUM), y0+0.065,
          length=0.06,angle=25 )
   txt = "Sum of 2\nrandom variables"
   if(!is.SUM) txt = "Difference of 2\nrandom variables"
   text(0 + 13*(1-is.SUM) , y0+0.065,txt,
        adj=c(1-is.SUM,0.5) )
   
   text(41*is.SUM -26*(1-is.SUM),
        y0+0.08, "====================" , 
        adj=c(is.SUM,0.5))
   
   text(31*is.SUM - 21*(1-is.SUM), y0+0.02, 
        bquote(3^2 ~  ~ + ~  ~ 4^2),
        adj=c(1-is.SUM,0.5),
        cex=1.5)
   
   txt = "VAR[Sum]"
   if(!is.SUM) txt = "VAR[Difference]"
   text(31.25*is.SUM - 20.5*(1-is.SUM), 
        y0-0.05,txt, adj=c(1-is.SUM,0.5))
   
   segments(41, 0.08, 37, 0.85,lwd=0.6 )
   segments(37, 0.08, 41, 0.85,lwd=0.6)

} # fn

```

```{r,eval=T, echo=F, fig.align="center", fig.height=4.5, fig.width=9, warning=FALSE, message=F,fig.cap="Variance (and thus SD) of the SUM of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the sum from its expectation is decomposed into its 2 components. This, (by the expansion rule for (a+b) squared that you learned in high school), each squared deviation becomes a sum of 2 squares, and a 'cross-product'. But the 4 cross-products cancel each other, and you are left with the sums of two squares, the original 3-squared and the  original 4-squared, i.e. the variances of RV1 and RV2."}

v=matrix(c(6,12,8,16),2,2)

sum.diff.2rvs(v)

```


Whereas one numerical example doesn't prove the 'the variances of a sum of independent RVs is the sum of their variances' rule, you can check out other more complicated examples with more values, and uneven distributions -- or use simulation -- and satisfy yourself that it is a general rule. Indeed, the formal mathematical statistics 'proof' uses the exact same method as that used in the panel, except that it replaces each number by a symbol!  (By the way, when students from other disciplines who claim to have taken statistics courses ask JH for permission to enrol in the math-stat based course bios601, one of his standard tests is to give then this problem: 'Let RV1 and RV2 be two random variables. From 1st principles, derive the formula for the variance of RV1+RV2'.)

So we can sum up the forgoing numerical example by saying:

$$ SD_1 = 3; SD_2 =4;\  but \ SD[Sum] \ \ne \ 3 + 4. \ \ \  SD[Sum] \ = \ \sqrt{3^2 + 4^2} = 5.$$  
**BOTTOM LINE : 3 + 4 is not 7! it's 5! ** Or, as physicists say, **Uncertainties ADD IN QUADRATURE'** (that's the  math word for the square root of the sum of squares). Now you can see why mathematical statisticians like to work with squared SD's. And you can see why we chose the 'nice' variance values 3 and 4. Just like for the right-angled triangles in Pythagoras' theorem, where the length of the hypotenuese is the square root of the squares of the elngths of the sides, so it is also with othogonal or independent random variables: the SD of their sum is the square root of the sum of the squared SD's of the individual RVs.

**And of course, since the theorem works for the sum of 2 independent RV's, it also works for the sum of 3, and for the sum of $n$ such RVs.**

### Measurement Errors

These are an important issue in the non-experimental -- and even the experimental -- sciences. The simplest case (called the **'classical' error model**) is where the errors are independent of the true value, so that the **variance in the observed (error-containing) values is the sum of the variance of the 'true' (errorless) values, and the variance of the errors.**

Even though many people think that random measurement errors **cancel out**, especially in large datasets, **they do not**. Even when they affect the $Y$ on the left side side of a regression model, they add 'noise' to the slope estimates. But when they affect an $X$ on the right hand side of a regression model, or even a correlation, their effects are  more  insidious. See for example, pages 19-21 of [these Notes](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/hw_measurement2019.pdf#page=19) and exercises 6, 8, 9, 18 and 21 that follow.

The other measurement error model, called 'Berkson' error, described [here:](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Surveys/EffectsXerrorsNotesFromALM.pdf), is less common, and has less nasty effects.

The following diagram shows the classical error model, and one of the important metrics to measure the extent of the error, namely the **intra-class correlation coefficient**. The 'ICC' is relevant no matter whether the variable is on the left or right had side of the regression model. Even though we named the random variable $Y$, it does not mean that measurement issues apply only to $Y$ variables. In fact, errors in $X$ variables have nastier effects. We chose the name $Y$ because we don't treat an errorless $X$ in a regression model as a random variable, and we are seldom interested in inferences regarding it!


```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap = "Random Measurement Error (E) added to a Random Variable. On the left is the distribution of a Random Variable Y, with each stickfigure representing a very large number of individuals. On the right is the distribution of the Random Variable Y', where Y' = Y + E, and E is independent of Y, and has a 2-point distribution, namely -0.5 and +0.5, with equal probabilities. (Here, the provenance/origin of each Y' value is shown by its colour, but in practice we would not have that luxury of knowing what the 'true' [errorless] value was). This is the variation we get to observe/measure. Of the variance of 1.93, some  1.68 of it is 'real'/'genuine', and the remainder, 0.25 is measurement error. The genuine variance of interest, 1.68, expressed as a proportion of the observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 or 87%. The proportion that is real is called the INTRA-CLASS CORRELATION (ICC) or intra-class correlation coefficient, and is an important indicator of the quality of the measurement of Y."}

par(mfrow=c(1,1),mar = rep(0,4))

XMAX = 25
plot(-10,-10,xlim=c(-6,XMAX),ylim=c(-1,15),
     xaxt="n",yaxt="n",frame=TRUE)

Y = seq(2,12,2)
fr =  2*c(1,2,4, 4,2,1)

text(-1,max(Y)+2.5,"Y",adj=c(1,0.5),cex=2,font=3)

abline(h=Y,col="lightblue")

Origin = matrix(NA,max(Y)-min(Y)+3,20)

source("./inst/code/drawStickPerson.R")

DX = 12
DY = 0.15

f.Y.star = rep(0,max(Y)-min(Y)+3)

for(i.r in 1:length(Y)){
  value = Y[i.r]/2
  text(-1,Y[i.r],toString(value),adj=c(1,0.5),cex=2)
  n = fr[i.r]

  for (j in 1:n){
      stickperson(j, Y[i.r]-2.5*DY, 0.15, DY, LWD=2,COL=i.r)
      i.y = Y[i.r] + ( (j%%2) == 0 ) - ( (j%%2) == 1 )
      J = min(  (1:20)[ is.na(Origin[i.y,])] )
      f.Y.star[i.y] = f.Y.star[i.y] + 1
      Origin[i.y,J] = i.r
      stickperson(DX+J, i.y - 2.5*DY, 0.15, DY, LWD=2,COL=i.r)
  } 
}

Y.star = NULL; 

for(i.r in 0:length(Y)){
   if(i.r == 0) { yy = Y[1] - 1   
                  y = Y[1]/2-0.5
                  txt = toString(y)
                }
   if(i.r >  0) { yy = Y[i.r] + 1 
                  y = Y[i.r]/2+0.5
                  txt = toString(y) 
   }
   Y.star = c(Y.star,y)
   text(XMAX,yy,txt,adj=c(1,0.5),cex=2,font=3)
   segments(DX, yy, XMAX, yy, col="lightblue")
}
text(XMAX,max(Y)+2.5,"Y '",adj=c(1,0.5),cex=2,font=3)

f.Y.star = f.Y.star[seq(1,length(f.Y.star),2)]
E.Y.star = sum(Y.star*f.Y.star)/sum(f.Y.star)
VAR.Y.star = sum((Y.star-E.Y.star)^2*f.Y.star)/sum(f.Y.star)
text( DX+ (XMAX-DX)/3, 0, 
      format(round(VAR.Y.star,2),nsmall=2),
      adj=c(0.5,1),cex=2)

Y = Y/2
fr =  2*c(1,2,4, 4,2,1)
E.Y = sum(Y*fr)/sum(fr)
VAR.Y = sum((Y-E.Y)^2*fr)/sum(fr)
text( DX/3, 0, 
      format(round(VAR.Y,2),nsmall=2),
      adj=c(0.5,1),cex=2)
text( -6, 0, "VAR:",adj=c(0,1),cex=2)


```

**The concept of an ICC depends on the law that 'variances add.'**


### Mean (of 2 or $n$ RVs)

AND, if we know how to compute the  SD of a SUM of $n$ independent RV's, we then automatically know  how to compute the  SD of a MEAN of $n$ independent RV's. Remember back to an assigment on the SD of a set of temperatures measured in the larger degrees F scale: then the
SD of the same set of temperatures measured in the smaller degrees C scale is just 5/9ths of the one in the F scale.

Going from a **sum** of $n$ RVs to a **mean** of $n$ RVs involves going to a scale that is (1/n)-th the spread of the sum scale.

So, in the above example, with $n$ = 2, the SD of the mean of the 2 RVs is (1/2) the SD of the sum, i.e.,

$$ SD\bigg(\frac{RV_1 +RV_2}{2}\bigg) = \frac{SD(RV_1+RV_2)}{2} = \frac{5}{2}.$$


**SPECIAL CASE** (quite common) where **SDs are identical**:

Up to now, to keep things general, we used $n$ non-identical -- but independent -- random variables. If we
consider the Variance and the sum of $n$ IDENTICAL -- and independent -- random variables,  so the the $n$ Variances (each abbreviated to Var) are all equal, the laws simplify

First, since the variances add, we have that

$$ Variance(RV_1 + RV_2 + \dots + RV_n) = Var_1 + Var_2 + \dots + Var_n = n \times \ each \ Var.$$

and so, taking square roots,

$$ SD( \ RV_1 + RV_2 + \dots + RV_n \ ) = \sqrt{ \ n \times \ each \ Var} = \sqrt{n} \ \times \ each \ SD$$

When we go from **sum** of $n$ IDENTICAL independent RVs to a **mean** of $n$ IDENTICAL RVs, we go to a scale that is (1/n)-th the spread of the sum scale. So, again, just as when we went from the larger degrees F scale to the smaller degrees C scale, we have

$$ SD\bigg(\frac{RV_1 + RV_2 + \dots + RV_n}{n}\bigg) = \frac{\sqrt{n} \ \times \ each \ SD}{n} = \frac{common \ SD}{\sqrt{n}} .$$
Sometimes, we will need to work with variances. When we do, we use this law: 

*$$ VAR\bigg(\frac{RV_1 + RV_2 + \dots + RV_n}{n}\bigg) = \frac{common \ VAR}{n} .$$

**EXAMPLE** Lengths of words in a book ("book A")

The number of dashes in of each row in the first panel  is the number of letters in a randomly selected word from the book: dashes are for better visibility. The words (rows) are sorted by length, form shortest to longest.  One cannot judge the full distribution just from this limited set, but (even though shape doesn't matter much in the big scheme of things) you get a sense of its shape. In the entire  book, the mean word length is about 4.5 letters, and the SD is
2.4 letters. (The fact that the distance between the minimum word length (1 letter) and the mean word length is less than 2 SDs hints that the full distribution has a longe right tail.) 

In each row in the coloured panels, some  4 (or 9) randomly sampled words are (like Galton's peas) pushed right up against each other, without spaces, and shown in  a mono-spaced font, so that where the 'line' ends indicates the total number of letters in the 4 (9) words. Since this small number of rows (possibilies) is too small to give a good sense of the sampling distribution, the smooth purple histograms were calculated exactly. 


```{r,eval=T, echo=F, fig.align="center", fig.height=12, fig.width=9, warning=FALSE, message=F, fig.cap="Illustrations of SD's of Sums and Means of n = 1, 4 and 9 independent and identically distributed random variables. Each RV is the length of a randomly selected word from a certain book. [Below, we will compare the mean word length in this book with the mean in abook by a competitor]. The distributions in purple were computed theoretically, using convolutions. Each row shows 1 'realization' of each of the  n random variables, with each word in a different color. The rows are sorted according to the values of the total [or mean] numbers of letters (chars) in the sample of n words. In the panels where n is 4 or 9, the leftmost n-1 characters of the n concatenated words are cropped, but the total/mean length  is correct. The top panel lists the 'per word' variation of all of the words in the book, and its SD, sometimes called the 'unit' variability. You can also think of the length of each unit as the mean of a sample of size n = 1. The second panel shows that to reduce the sampling variation (the SD) of the mean by half, one needs to quadruple the n. The third panel shows that to reduce the sampling variation (the SD) to 1/3, one needs to multiply the n by 9. Note, in passing, that at $n$ = 9, via the Central Limit Theorem, and the fact that the original distribution is 'CLT friendly' (the mode is not at either extreme, and the tails don't extend indefinitely), the shape of the sampling distribution is already close to Gaussian."}

ds = read.table(
"http://www.biostat.mcgill.ca/hanley/statbook/WH.txt.words.txt",
as.is=TRUE)

L.max = 12

w = ds[ nchar(ds$Word) <= L.max, ]

CEX=1

Y = as.numeric( nchar(w) ) 
E.Y = mean(Y)
SD.Y = sd(Y)
VAR.Y = var(Y)
Max.Y = max(Y)

Fr = table(Y)[1: L.max]
Fr = Fr/sum(Fr)

to.show = c(1,4,9)
Max.n   = max(to.show)

P = c(0.025,0.975)

n.rows=25

par(mfrow=c(length(to.show),1), mar = rep(0,4))

for (n in 1:Max.n){
	
	if(n==1) { 
		Prev = Fr; max.y = L.max; range = 1:max.y 
	}
	
	if(n >1) {
		M = outer(Prev,Fr)
	    Prev = sapply(split(M, col(M) + row(M)), sum)
	    range = n:(n*max.y)	
	}
	
	if(n %in% to.show){
		
		XLIM = c(n-1, 1.05*L.max*n)
		YLIM = c(-0.08,1)*max(Prev)
        plot(range,Prev, type="h",
             yaxt="n", xaxt="n",
             xlim=XLIM, ylim=YLIM, 
             lwd=1, main=toString(n),col="white")
        
        for(s in range ) if( (n*floor(s/n)) ==s){
          text(s, -0.02*max(Prev),toString(s),
               cex=1.5,adj=c(0.5,1))
          text(s, -0.07*max(Prev),toString(s/n),
               cex=1.5,adj=c(0.5,1),font=2)
        }
        text(1.025*max(range), -0.02*max(Prev),"SUM",
               cex=1.5,adj=c(0,1))
        text(1.025*max(range), -0.07*max(Prev),"MEAN",
               cex=1.5,adj=c(0,1),font=2)
        
        text((9/10)*max(range),0.74*YLIM[2],
             paste("SD of \nTOTAL Length\nof n =",
                   toString(n),
                   "Word(s):\n\n",
                   format(round(SD.Y*sqrt(n),1),nsmall=1),
                   "chars"),
             cex=2)
        
        text((9/10)*max(range),0.3*YLIM[2],
             paste("SD of \nMEAN Length\nof n =",
                   toString(n),
                   "Word(s):\n\n",
                   format(round(SD.Y/sqrt(n),1),nsmall=1),
                   "chars"),
             cex=2,font=2)
        
        dx = strwidth("1", units = "user", 
                cex = CEX, font = 1, family= "mono")

        
        segments(XLIM[1], 0, XLIM[2], 0)
        points(n*E.Y + SD.Y* qnorm(P)*sqrt(n), rep(0,2),pch=19,cex=.5)
        
        dy = YLIM[2] / n.rows
        
        mm = matrix(sample(w, n.rows*n,replace=TRUE),
                     n.rows,n)
        L = apply(nchar(mm),1,sum)
        mm=mm[order(L),]
        for(i.r in 1:n.rows){
            if(n >1) Words  = mm[i.r,]
            if(n==1) Words  = mm[i.r]
            xx = 0.5
            for(i.w in 1:n) {
               txt = Words[i.w]
               if(n==1) txt = paste(
                  rep("-",nchar(txt)),collapse="")
               text(xx,i.r*dy, txt,
                 family="mono",
                 adj=c(0,0.5),cex=1/dx,
                 col=i.w)
               xx = xx+nchar(txt)
            }
        } # i.r
        lines(range,Prev, type="h",
              lwd=3, col="purple")
		
	} # show
   
} # n


```

As you would have expected, the  purple **sampling distributions  narrow with increasing sample size**, but the narrowing is **not by a factor of $n$,** but by a **factor of $\sqrt{n}$.** All the billions of _possible means_ of samples of size $n$ = 4
_would have_ a SD of 2.4/$\sqrt{4}$ = 2.4/2 = 1.2. The _possible means_ of samples of size $n$ = 9 _would have_ a SD of 2.4/$\sqrt{9}$ = 2.4/3 = 0.8.
**Note the careful choice of the words** in _italics_: in reality, you will only observe **one** of the billions of possibilities, so the sampling distribution is _imaginary_ and thus the SD is also _imaginary_ and so the SD of the **conceptual** sampling distribution is (_would be_) an _imaginary_ 1.2 or 0.8. The only reason we are able to show the purple distributions is because of the laws of statistics, applied to all the words in the book, so we know the mean and the unit SD.




```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

fr = c(15273, 11193, 8226, 5761, 3300, 1397, 404, 97)

proportion.of.households =round(fr/sum(fr),3)
n.children = 1:8

# noquote(format(outer(n.children,n.children,"+"),w=8))

#outer(proportion.of.households,proportion.of.households)

```

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

fr = c(15273, 11193, 8226, 5761, 3300, 1397, 404, 97)

proportion.of.households =round(fr/sum(fr),3)
n.children = 1:8

# noquote(format(outer(n.children,n.children,"+"),w=8))

#outer(proportion.of.households,proportion.of.households)

```

### Difference of 2 RVs


To keep it simple, let's consider the two very simple random variables (RV's), each taking just 2 values, and with equal probabilities, and independent of each other. But suppose now that we are interested in their difference


$$Difference = RV_1 \ (6 \ or \ 12) \ - \ RV_2 \ (8 \ or \ 16) \ = \ -10 \ or \ -4 \ or \ -2 \ or \ +4.$$  

Now, imagine taking a random value of $RV_1$ and  subtracting from it a random value of $RV_2$. There are 4 possible differences, and in this deliberately constricted example, they are all distinct. A probability tree (next panel) helps to see the 4 possibilities, and the lengths of the branches denote the values of the RVs.

The 4 equally likely differences are -10, -4, -2 and +4, and so their mean is $\frac{-10 -4 -2 +4}{4} = -3.$ That the mean (expected value) of the difference equals the difference of the 2 individual means or expected values, is hardly surprising [ -- and it is even true if the 2 RV's were not independent.] The 4 equally likely deviations from this -3 are -7, -1, +1, and +7, and so, **from 1st principles**, the **Variance of the difference of the 2 RVs is** 
$$\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \frac{100}{4} = 25.$$  
Thus, the **SD of the difference of the 2 RVs is** $\sqrt{25}$ = 5.

Fortunately, we don't have to go back to 1st principles **to calculate the SD of the differences of 2 RVs -- but we do NOT do so by adding the 2 SDs**. IT IS THE VARIANCES THAT ADD! 

```{r,eval=T, echo=F, fig.align="center", fig.height=4.5, fig.width=9, warning=FALSE, message=F,fig.cap="Variance (and thus SD) of the DIFFERENCE, RV1 - RV2, of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the difference from its expectation is decomposed into its 2 components. Each each squared deviation becomes a sum of 2 squares, and a 'cross-product'. But the 4 cross-products cancel each other, and you are left with the SUMS of two squares, the original 3-squared and the  original 4-squared, i.e. the variances of RV1 and RV2."}

v=matrix(c(6,12,-8,-16),2,2)

sum.diff.2rvs(v)

```


Again one numerical example doesn't prove the 'the variances of a difference of two independent RVs is the sum of their variances' rule, but you can check out other more complicated examples with more values, and uneven distributions -- or use simulation -- and satisfy yourself that it is a general rule.

So we can sum up the forgoing numerical example by saying:

$$ SD_1 = 3; SD_2 =4;\  but \ SD[Difference] \ \ne \ 3 + 4. \ \ \  SD[Difference] \ = \ \sqrt{3^2 + 4^2} = 5.$$  
It turns out that, from what we already knew about the sum of 2 independent RVs, we have anticipated this law. We didn't need to go through all the formulae from scratch again.  The reason had to do with the 'mirror image' distributions, such as the depths of the ocean, we saw above. The spread (SD, or variance) is the same, whether one writes/reads/computes from left to right, or right to left! In other words, the variance of the random variable $-RV_2$ is the same as that of the random variable $RV_2.$ So, by writing $RV_1 - RV_2$ as a sum, and using the law for the variance of a sum, we arrive at 
$$Var[RV_1 - RV_2] = Var[RV_1 + (-RV_2)] = Var[RV_1] + Var[(-RV_2)] = Var[RV_1] + Var[RV_2].$$


**EXAMPLE** Difference in mean length of words in books A (in blue) and B (in red)

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="Differences in  mean lengths of  n randomly selected words from each of 2 books. The (sampling) distributions were computed theoretically, using convolutions."}

dsA = read.table(
"http://www.biostat.mcgill.ca/hanley/statbook/WH.txt.words.txt",
as.is=TRUE)

dsB = read.table(
"http://www.biostat.mcgill.ca/hanley/statbook/AD.txt.words.txt",
as.is=TRUE)

L.max = 12

wA = dsA[ nchar(dsA$Word) <= L.max, ]
wB = dsB[ nchar(dsB$Word) <= L.max, ]

FrA = table(nchar(wA))[1: L.max]
FrA =  FrA/sum(FrA)

Y = 1: L.max

mA = sum(Y*FrA)
vA = sum((Y-mA)^2 * FrA)
sdA = sqrt(vA)

FrB =table(nchar(wB))[1: L.max]
FrB = FrB/sum(FrB)
mB = sum(Y*FrB)
vB = sum((Y-mB)^2*FrB) 
sdB = sqrt(vB)

Fr = matrix(c(FrA,FrB),L.max,2)

CEX=1

to.show = c(5,25,100,400)
to.show = c(5,10,25,50)   # for now
Max.n   = max(to.show)

par(mfrow=c(length(to.show),1), mar = rep(0,4))

for (n in 1:Max.n){
	
  if(n==1) { 
		PrevA = Fr[,1];
		PrevB = Fr[,2] 
		max.y = L.max;
		range = 1:max.y 
	 }
		
	 if(n >1) {
		MA = outer(PrevA,Fr[,1])
	    PrevA = sapply(split(MA, col(MA) + row(MA)), sum)
	    MB = outer(PrevB,Fr[,2])
	    PrevB = sapply(split(MB, col(MB) + row(MB)), sum)
	    range = n:(n*L.max)	
	 }
	
	if(n %in% to.show){
		
		XLIM = c(n-1, 1.001*L.max*n)
		YLIM = c(-0.10,1.05)*max(PrevA)
        
        plot(range,PrevA, type="h",
             yaxt="n", xaxt="n",
             xlim=XLIM, ylim=YLIM, 
             lwd=1,col="white")
        
        for(s in range ){
            Int.part = floor(s/n)
            if ( (n*Int.part) ==s & Int.part < 7 ) text(
                   s, -0.04*max(PrevA),toString(s/n),
                  cex=1.25,adj=c(0.5,1),font=2)
        }
        
        lines(range,PrevA, type="h",
              lwd=0.25, col="blue" )
        lines(range+0.2,PrevB, type="h",
              lwd=0.25, col="red" )	
        
        MAB = outer(PrevA,PrevB)
        F = sapply(split(MAB, row(MAB) - col(MAB)), sum)
        diff.sum = max(range) - min(range)
        diff.sum = (-diff.sum):diff.sum
        offset = n*9
        text(1.1*n, 0.005*max(PrevA),
        paste(
         "possible\nmean\nno. of letters\nin samples\nof size n = ",
        toString(n)),
               cex=1.25,adj=c(0,-0.1),font=2)
        lines( (offset + diff.sum), F, 
               type="h", col="grey60", lwd=0.5)
        segments(offset, 0,offset, max(F),
                 col="purple",cex=3)
        for(dif in seq(-2,2,0.5) ) {
          text(offset + dif*n, 
               -0.04*max(PrevA),
               toString(dif),
               cex=1,adj=c(0.5,1),font=4)
        }
        
        text(0.99*n*L.max, 
             0.005*max(PrevA),
      "possible\ndifferences\nbetween these\nsample means",
             cex=1.25,adj=c(1,-0.1),font=4)
        
        text(5.5*n,1.04*max(PrevA),"A",
             adj=c(0,1),cex=1.5)
        text(  3*n,1.04*max(PrevA),"B",
             adj=c(0,1),cex=1.5)
        text(  9*n,1.04*max(PrevA),"A-B",
             adj=c(0,1),cex=1.5)
        
        text(7*n,0.9*max(PrevA),"<- VAR - >",
             adj=c(0,1),cex=1.5)
        
        text(5.5*n,0.9*max(PrevA),
             format(round(sdA^2/n,2),nsmall=2),
             adj=c(0,1),cex=1.5)
        
        text(3*n,0.9*max(PrevA),
             format(round(sdB^2/n,2),nsmall=2),
             adj=c(0,1),cex=1.5)
        
        text(9*n,0.9*max(PrevA),
             format(round(sdA^2/n + sdB^2/n,2),
                    nsmall=2),
             adj=c(0,1),cex=1.5)
        
        
        text(7*n,0.75*max(PrevA),"<- SD ->",
             adj=c(0,1),cex=1.5)
      
        
        text(5.5*n,0.75*max(PrevA),
             format(round(sdA/sqrt(n),2),nsmall=2),
             adj=c(0,1),cex=1.5)
        
        text(3*n,0.75*max(PrevA),
             format(round(sdB/sqrt(n),2),nsmall=2),
             adj=c(0,1),cex=1.5)
        
        text(9*n,0.75*max(PrevA),
          format(round(sqrt(sdA^2/n+sdB^2/n),2),
                    nsmall=2),
             adj=c(0,1),cex=1.5)
        
	} # show	
	 
} # n

```

## Linear combinations of RVs (regression slopes)

In _non-experimental_ research especially, the focus is typically on a fitted regression slope/coefficient, rather that on the simple difference 
$\bar{y}_1$ - $\bar{y}_0$ between the means of the $y$s observed at each of two investigator-chosen values ($X=1$ and $X=0$) of the determinant ($X$) being studied. 

Even if the estimator  does not have a closed form, the fitted slope(s)/coefficient(s) are linear combinations of the $y$'s and the $x$'s. Thus, since each of the $n$ $y$'s contains a random element, the slope ($\hat{\beta}$) is an $x$-based linear combination of $n$ random variables.

Thus one can view all variances (and thus all standard errors) in a unified way, and not have to learn separate laws for separate chapters. To see how this unified view avoids the typical 'silo' approach to statistical tecnniques, see [Sample Size, Precision and Power Calculations: A Unified Approach](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/UniversalSampleSize.pdf). [Software developers who thrive on separate 'niche' markets are threatened by this parsimonious approach, just as are those who write 800-page textbooks with separate chapters for t-tests,l proportions, regression, multiple regression, logiostic regression, Cox regression, survival analysis,etc.]    

In the past, when first introduced to simple linear regression, it was common to learn the estimator formula and the Variance formula by heart, and use them to compute the fitted slope and the the standard error for a fitted slope by hand,
$$\hat{\beta} = \frac{\sum (y-\bar{y})(x-\bar{x})}{ \sum (x-\bar{x})^2} \ ; \ \ Var[ \hat{\beta} ] = \frac{\sigma_e^2}{ \sum (x-\bar{x})^2} \ ; \ SE[ \hat{\beta} ] \ = \sqrt{Var[ \hat{\beta} ]} \ .$$
In the variance formula, $\sigma_e^2$ is the variance in the 'errors' in the $y$'s. In practice, we have to estimate this quantirty, but in our example, for disdactic purposes, we will pretend to 'know' its value.

Sadly, [these formidable formulaa hide what is going on](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/SimpleMultipleLinearRegressionSampleSize.pdf). 

To truly understand what is going on, lets consider a **very simple example** where we can _see_ what is happening. A student from a country that uses the Fahrenheit (F) system moves to Montreal, and wishes to know how to translate the outside temperature, expressed as the number of degrees Celsius (C) and shown in the Metro, and heard on radio stations, back into the F scale (s)he is familiar with. The student knows that the conversion is of the form  F = $\alpha$ + $\beta \times C$ but, rather than look them up on Google, decides to estimate $\alpha$ and $\beta$ from pairs of (C,F) readings, taking the C readings directly from the Metro screen, and the F ones from his/her own portable thermometer. Suppose that the C readings are displayed to 1 decimal place, but that the F thermometer only displays the F to the nearest integer.

Now, knowing that one it just takes 2 data points to draw a line, the student takes F measurements on two occasions, once when the displayed temperature is 12.5 C and one when it is 17.5 C.  (The student didn't know that when the real temperature  was very close to xx.5 F, it had a 50% chance of being rounded up to xx+1 F, thereby creating an error of +0.5 F, and a 50% chance of being rounded down to xx-1 F, and  producing an error of -0.5 F. Thus, the variance of each error is $(-0.5)^2 \times (1/2)$ + $(+0.5)^2 \times (1/2)$ = $0.5^2,$ and the SD is 0.5. (In the computer exercises, we will treat a broader type of  random errors in the F readings).

Given that these two C settings correspond to 54.5 F and 63.5 F, but that the true temperature may be slightly on one side or the other of thse two values, what are the possible $\beta$ estimates? And, how variable would they be?

The 4 possibilities, shown as the slopes of the 4 fitted lines shown in black below, are (64-55)/5, (64-54)/5, (63-55)/5, and (63-54)/5, or, when sorted, 1.6, 1.8, 1.8 and 2.0 degrees F per degree C, each with probability 1/4, so that the variance of the equally likely slopes is
$$Var[ \hat{\beta} ] =  \frac{(1.6 - 1.8)^2 + (1.8 - 1.8)^2  + (1.8 - 1.8)^2 + (2.0 - 1.8)^2}{4} = \frac{1}{50} \ =  \ 0.02,$$
and the SE is $\sqrt{0.02} = 0.14$ degrees F per degree C.

Also shown in the diagram is the **'anatomy' of the 'random slope'**. The possible slopes are displayed as a single expression  in which the 2 random elements (i.e the 2 random  variables, or the two 'errors' e$_1$ and e$_2$)  are isolated. The random slope is in the form of a constant (9/5) plus another constant (1/5) times the difference of two independent random errors. Applying all of the variance rules above, we have that
$$Var[random  \ slope] = (1/5)^2 \times ( Var[e_1] + Var[e_2])  \ = \ \frac{1}{50} \ .$$ 

```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="The 4  lines (in black) fitted to the 4 possible and equally likely pairs of data points (The  <unknown to us> 'true' values are shown in blue). By algebraicly isolating the contributions of the 2 random errors in F to the variation in the 4 slopes, the variance of the (random) slopes is easily computed using the laws for the variance of a combination of 2 independent random variables."}


par(mfrow=c(1,1), mar = rep(0,4))

x = c(12.5,17.5)
Y = 32 + (9/5)*x

XLIM = c(x[1]-1, x[2]+1)
YLIM = c(Y[1]-2, Y[2]+2)

plot(-100,-100,xlim=XLIM,ylim=YLIM)

yy  = seq( floor(YLIM[1]), ceiling(YLIM[2]),1)
yyy = seq( floor(YLIM[1]), ceiling(YLIM[2]),1/2)
abline(h=yy, col="lightblue",lwd=1.5)
abline(h=yyy,col="lightblue",lwd=0.52)

for(Yy in yy) text(XLIM[1],Yy,
                  toString(Yy),
                  adj=c(1,1) )

text(XLIM[1],YLIM[2] - 0.25, "Temperature (F)",
                  adj=c(0,0),font=2)

text(mean(x),YLIM[1]-0.25, "Temperature (C)",
                  adj=c(0.5,0),font=2)

H = seq(floor(XLIM[1]),ceiling(XLIM[2]),1)
for(h in H ) text(h,YLIM[1]+0.25,
                  toString(h),
                  adj=c(0.5,0))
abline(v=H,col="lightblue")
segments(x[1],Y[1],x[2],Y[2],col="blue")

e = c(-0.5,0.5)
for( i.1  in 1:2  ){
  y1 =  Y[1]+e[i.1]
  text(x[1],y1,toString(y1),adj=c(1.25,0.5))
  for( i.2  in 1:2  ){
     y2 =  Y[2]+e[i.2]
  	 if(i.1==1) text(x[2],y2,
  	               toString(y2),adj=c(-0.25,0.5))
  	 dy = y2-y1
  	 slope = dy/(x[2]-x[1])
     segments(x[1], y1, x[2], y2, lwd=0.5 )
  }
}

y = Y[2]

  text(x[1],y,
    bquote("slope = " ~ frac(y[2]-y[1],5) ~ " = " ~ frac(63.5+e[2] - 54.5 - e[1],5 ) ~ " = "  ~ frac(9 + e[2] - e[1],5 ) ~ "  =  " ~ frac(9,5) + frac(e[2] - e[1],5)  ),
    adj=c(0,0.5)
  )

points(x[1],Y[1],cex=0.5,pch=19,col="blue")
arrows(x[1],Y[1]-1/2, x[1],Y[1]+1/2,
       length=0.06, angle=45,code=3,lwd=1)
points(x[2],Y[2],cex=0.5,pch=19,col="blue")
arrows(x[2],Y[2]-1/2, x[2],Y[2]+1/2,
       length=0.06, angle=45,code=3,lwd=1)
text(x[1]-1/5,Y[1],
    bquote(e[1]),  adj=c(1,0.5), 
    cex=1)
text(x[2]+2/5,Y[2],
    bquote(e[2]),  adj=c(1,0.5), 
    cex=1)
text(x[1]-4/5,Y[1]+4.5,
  bquote("Var[" ~ e[1] ~ "] = " ~ frac( (-0.5)^2 + (+0.5)^2,2) ~ " = " ~ 0.5^2 ),
  adj=c(0,1), cex=1)

text(x[2]+1,Y[2]-3.25,
  bquote("Var[" ~ e[2] ~ "] = " ~ frac( (-0.5)^2 + (+0.5)^2,2) ~ " = " ~ 0.5^2 ),
  adj=c(1,1), cex=1)


text(x[2]+1,Y[1],
    bquote("Var[possible slopes] = Var[ " ~ frac(9,5) + frac(e[2] - e[1],5) ~ " ] = " ~ frac(Var(e[1])+Var(e[2]),5^2) ~ " = " ~ frac(0.5^2 +0.5^2,5^2) ~ " = " ~ frac(1,50) ),
    adj=c(1,0.5)
  )
  
```

The important point of this simple regression example is that even though in practice there would be many more data points, the principle/law used to calculate the sampling variation of a slope based on any number of datapoints remains the same: the fitted slope is still an $x$-based linear combination of $y$'s, (in this case, a closed form combination)  and thus an $x$-based linear combination of random errors -- or more broadly of random deviations from the $x$-conditional means of the '$Y$' variables. We will return later to all of the factors that influence the narrowness/width of  sampling distributions generally, but you can maybe already see from the 'algebraicly isolated' representation of the slope that the more datapoints -- and the wider apart they are on the $x$ axis and the smaller the magnitudes of the errors --- the more reproducible the slope. The influence of this latter factors is less evident in the textbook formula for the Variance and the SE. This little exercise (next) should help you figure out how the factors come into it. [This piece](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/UniversalSampleSize.pdf) also focuses on these isssues in a transparent way. See the exercise on this.



## Exercises

1. Refer to the **fictitious cohort** (shown above), constructed from the 1990 Quebec mortality rates.

   + Use  1st principles (together with `R`) to calculate the standard deviation of the longevity of the male cohort. Do so in two ways, using (a) the definition (b) the 'shortcut'.
   + For [human 'computers'](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Hanley_Article_Galton_Data.pdf#page=7) back in the days before there were 'electronic' computers, what is the advantage of the shortcut?

2. Suppose you get into the **life insurance** business in a small way, just taking on one client. The client pays you a premium of $100 at the beginning of each year for 5 years. If the client dies within the next 5 years, you will pay client's estate $20,000. Thus, at the end of 5 years, your possible earnings from this single client, along with the associated (actuarily-based) probablities are:


```{r,eval=T, echo=T, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

possible.earnings = c( seq(-19900,-19500,100), 500 ) 
probability = c(183,186,189,191,193,99058)/100000 

cbind(possible.earnings,probability)
      
``` 
2. continued

   + Compute the expected earnings
   + Compute the variance (and thus the SD) of the possible earnings (a) using the definition (b) using the computational shortcut
   + Compute the 'risk',  the SD as a percentage of the mean, as do investors ranking how risky various stocks are.
   + In statistics, and especially in applied statistics, what is the name for the SD as a percentage of the mean?

3. **Errors caused by rounding**. Suppose one has to analyze a large number of 3 digit numbers. To make the job easier, one rounds each number to the nearest 10, e.g.,  
`460 <-- 460 461 462 463 464 ; 465 466 467 468 469 --> 470.`  
If the ending numbers of the unrounded data were uniformly distributed (each ending digit has a probability of 1/10), calculate:
   + the average error per (rounded) number
   + the average absolute error per (rounded) number
   + the square root of the average squared error per (rounded)
number ['root mean squared error', or 'RMSE' for short]


4. **Correcting for guessing** on multiple choice exams.  
Suppose one wishes to estimate via a multiple choice examination [with $k$ answers to choose from for each question], what proportion $\pi$ of questions a student **knows** the answer to (excuse the dangling preposition!). Imagine that $\pi$ refers to the N (>> n)   questions in the much larger bank of questions from which the $n$ exam questions are randomly selecetd. 

   + Show that the simple proportion $p$ of correctly answered questions gives a biased (over) estimate of $\pi$ if the student simply randomly guesses among the $k$ answers on questions where (s)he doesn't know the answer. Do this by calculating the expected value of p (i.e. the average mark per question) when each answer is marked 1 if correct and 0 if not. (_Hint_: a tree diagram may help).
   + One can 'de-bias' the estimate by giving each correct answer a mark of 1 and each incorrect  answer a negative mark. What negative mark (penalty) will provide an unbiased estimate of $\pi$? Begin by finding the expected mark per question, then set it to $\pi$ and solve for the penalty. (_Hint_: If you prefer, use concrete values of $\pi$ and $k$ to see what penalty is needed.)
   
5. Half the purchases of eggs in a market are for 6 eggs and half are for 12. What percentage of purchases are for a quantity that is more than 1 SD from the mean? less than 1 SD?

6. Half the people in a population have 2 organs of a certain type and half have none. What is the standard deviation of the number of organs a randomly selecetd person has? 

7. Verify the variances displayed in the above Figure showing the distribution of a random variable, before and after measurement errors are added to it.  Then subtract the smaller variance from the larger one to estimate the error variance. Finally show, by a separate calculation, why your answer 'fits' with the details of how the error-containing variable was constructed.  

8. Consider children of parents who both carry a single copy of the CF gene. (In the absence of ..) How many of their offspring will have 0, 1 or 2 copies? 

9. Half of a large number of orders were placed on Tuesday and half on Thursday. The combined orders were all jumbled together and shipped in 3 equal sized shipments on Monday Wednesday and Friday of the following week, arriving the same day they shipped. Calculate  the mean and standard deviation of the number of days between ordering and arrival. Use a probability tree to depict the randomness, and to show the calculations.


10. Refer to the example where the student tries to estimate the scaling factor between degrees C and degrees F.
   + What if the student took the F readings at two C values that are 10 C (rather than 5 C) apart?, i.e. at 12.5 C and 22.5 C?
   + How would the Variance and the SE be altered?
   + What if the student took the F readings at four equally spaced C values 5 C apart, i.e., at 7.5 C, 12.5 C, 17.5C and 22.5 C?.
   +If you were limited to $n$ C values, how would you decide where to place them?
   + What if, rather than $0.5^2$, the 'errors' in F had a variance of $1^2$ or $2^2$?

UNDER CONSTRUCTION

* Elevators (needs normal)

* Trial of Pyx
  

## Summary Slides

* The concepts of a random variable, and of its expectation and variance, underpin all of statistical inference. That is why this chapter is so central, even if we don't apply the laws by hand. 

* The laws governing the variance of the sum and the mean of $n$ random variables are the basis for Standard Errors (SEs) of statistics (parameter estimates based on aggregated data).  

* When assessing the sampling variability or a sum or mean of independent random variables, it is not their standard deviations that sum (add), but rather their variances.

* This is why we have the $\sqrt{n}$ law in Statistics. The  SE of a statistic is directly proportional to $\sqrt{n}$ if we are dealing with a sum, and inversely proportional to $\sqrt{n}$ (or  proportional to 1/$\sqrt{n}$ ) if we are dealing with a mean.

*  Since proportions are means (albeit of RVs that just take on two possibvle values), the same laws apply to them as well. 

* This law was not understood/appreciated until recent centuries. Statistical historial Stephen Stigler has a very nice example, in [this article](http://www.medicine.mcgill.ca/epidemiology/hanley/c323/pyx.pdf), and retold in his book [The Seven Pillars of Statistical Wisdom](https://www-degruyter-com.proxy3.library.mcgill.ca/view/title/521193), where failure to understand it  wrong gave people quite a bit of leeway to cheat.   

* It's also why statisticians are forced to work with variances when developing the properties of estimators. But as an end user, you will typically work with the square roots of these, and speak about the number of children per parent rather than  square children per square parent.

* The law governing the variance of a difference of two random variables is even more important, since we are more often interested in contrasts than the level in a single context.  

* Whether we are add or subtract independent random variables, the result is more variable than its parts. 

* A regression slope can be represented as a linear combination (with varying positive and negative combining weights) of random variables, and so the variance and SD of the sampling distribution of a slope are gioverned by these same fundamental laws.

* Although the main focus was on Variances and SDs, along the way in these above sub-sections, you saw the Central Limit Theorem (CLT)  trying to exert itself. Although the narrowness/width of a sampling distribution is measured by a variance or SD, the _CLT focuses more on its shape_. It is not possible to give a general rule for the $n$ at which the CTL will ensure a sufficiently Gaussian shape for a sampling distribution. How 'close' to Gaussian any particular sampling distribution is depends on the 'parent' RV and the $n$, but also on what you consider is 'close enough for Government work'.        

* It is not critical that we 'do' several exercises on the theory (laws) in this chapter. After all, you will seldom have to manually do the SE calculations based on these laws -- the statistical software will do it for you. But, you do need to understand the factors that make the SE's big or small, and the concepts involved in the propagation -- and reduction -- of errors.   There are several exercises in the computing session that will allow you to 'see' the laws in action, so that you can adopt them as guiding principles for study design, and for appreciating the 'precision' with which you can chase (take a shot at) statistical parameters.

* For now, because the chapter is already a long one, we didn't address sums and differences of _non-independent_ random variables. But some of the computer exercsies will.

