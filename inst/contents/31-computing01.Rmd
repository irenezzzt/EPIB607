# Computing Session 1 {#computing01}

## Objectives

The **'computing' objectives** are to learn how to use `R` to put series of observations into vectors, and how to plot one series against another.

The **'statistical' objective** of this exercise is to understand the concept of a distribution of a numerical characteristic (here an amount of elapsed time), and the various numbers describing its 'central' location and spread, and other 'landmarks'. You will also be introduced (in the next section) to 2 functions that give a more complete description of a distribution.

## Background to two datasets 

### Climate

Later on, when we get to regression models, we will examine [climate trends using unusual datasets](http://www.medicine.mcgill.ca/epidemiology/hanley/c678/index.html#anthropometricData), which suggest that over the last few centuries, winter tends to end earlier, and plants tend to flower earlier.

One such dataset arose as part of a long-running contest, the [Nenana Ice Classic](http://www.nenanaakiceclassic.com)
More [here](http://www.john-daly.com/nenana.htm).

Here is the [2020 brochure](http://www.medicine.mcgill.ca/epidemiology/hanley/bios691/Nenana2020Brochure.pdf). When this exercise was constructed, in March 2020, is was time to compete this year, but it looked like you needed to be in Alaska to find and fill out  a ticket, and  the organizers asked for your mailing address, not your email one!

### Ages of Cars

One measure of the state of, and confidence in, a country's economy is the monthly, or quarterly or yearly, numbers of new car registrations, such as these data from
government 
[e.g., ](https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=2310006701) and commercial sources, e.g., for
[Canada](https://tradingeconomics.com/canada/car-registrations),
[the USA](https://tradingeconomics.com/united-states/car-registrations), and the 
[UK](https://tradingeconomics.com/united-kingdom/car-registrations).

Another, which we will explore here, is the age distribution of all registered cars. (Because it is the only one we could find with detailed age data) we will examine a dataset ('VEH0126') that contains the numbers, as of the end of 2019, of Licensed vehicles by make and model and year of manufacture, produced by the [UK Department for Transport](https://www.gov.uk/government/statistical-data-sets/all-vehicles-veh01) 


## Statistical/Computing Tasks

### Guesses re Date of Ice Breakup

You are asked to approximate and carefully examine the distribution of guesses in 2018, contained in the Book of Guesses for that year. The full book is available  as the 7th tab in the [2020 website](http://www.nenanaakiceclassic.com). If the full book takes too  long to load, here are some 
[excerpts from the book](http://www.medicine.mcgill.ca/epidemiology/hanley/bios691/SampledPages.pdf).

For now, we will measure the guesses (and eventually the actual time) as the numbers of days since the beginning of 2018. Thus a guess of Tuesday April 17 5:20 p.m. would be measured as 31 + 28 + 31 + 16 + (17 + 20/60)/24 = 106.7208 days since the beginning of 2018.

It would be tedious to try to apply optical character recognition (OCR) to each of the  1210 pages in order to be able to computerize all of the almost 242,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways: 

1. By determining, for  each  of the 36 x 2 = 72 half-days days from April 10 to May 15 inclusive, the proportion, p, of guesses that are earlier than mid-day and midnight on that date. [ In `R`, if p = 39.6% of the guesses were below xy.z days, we would write this as pGuessDistribution(xy.z) = 0.396. Thus, if we were dealing with the location  of a  value in a Gaussian ('normal') distribution, we would write `pnorm(q=110, mean = , sd = )` ] Once you have determined these 72 proportions (p's), plot them on the vertical axis against the numbers of elapsed days since the beginning of the year on the horizontal axis. Thus the horizontal axis runs from 92 + 10 = 102 days to 92 + 30 + 15 = 137 days.

2. By determining the 1st, 2nd, ... , 98th, 99th percentiles. These are specific examples of 'quantiles', or q's. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all guesses are below this value, and 1-q are above it. [ In `R`, if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a  Gaussian distribution with mean 130 and standard deviation 15, we would write `qnorm(p=0.4, mean = 130, sd = 15)`. ] Once you have determined them, plot the 99 p's (on the vertical axis) against the 99 (elapsed) times on the horizontal axis. [see the spreadsheet].

### How old are UK cars?

Below, you are asked to summarize the age distribution of all of the cars registered in 2019. Since the datafile (which you can open in OpenOffice or in Excel) has separate sheets for different types of vehicles (cars, motorcycles, buses, etc.) you will need to save the 'cars' sheet as a '.csv' (comma separated values) file. Besides some header material (which you can skip using the `skip` argument in the `read.csv' command in `R`) it has as may rows as there are models  (approx 40,000), and a 'count' column for every year of manufacture, back to 1900 or so, as well as a total across all of these years.k  


**But first, before we get to the actual exercises, an orientation ...**


## The p and q functions: an orientation

The '**p**' function  tells us, for a given value of the characteristic, what **p**roportion of the distribution lies to the left of this specified value.

The '**q**' (or quantile) function  tells us, for a given proportion p, what is the value of the characteristic such that that specified proportion p of the distribution lies to the left of this 'q' value.

In the plot below, the values of the **p** function are shown on the vertical axis, in red, against the (in this case, equally-spaced) values of the characteristic, shown on the horizontal axis. You enter on the horizontal axis, and exit with an answer on the vertical axis.

The **q** function (in blue) goes into the opposite direction. You enter at some proportion on the vertical axis, and exit with a value of the characteristic (a quantile) on the horizontal axis. In our plot, the proportions on the vertical axis are  equally-spaced. Percentiles and quartiles are a very specific sets of quantiles: they are obtained by finding the values that divide the distribution into 100 or into 4.  

```{r,eval=T, echo=F, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F}

SHAPE = 5; SCALE=1.4
values.of.characteristic = seq(0,25,1)
p = pgamma(values.of.characteristic,shape=SHAPE,scale=SCALE)
par(mfrow=c(1,1),mar = c(5,5,2,0.1))
plot(values.of.characteristic,p,type="l",
     cex.lab=2,
     xlab="Value of characteristic",
     ylab="p: proportion < indicated value",
     ylim=c(-0.015,1), xlim=c(-1,25),
     cex.axis=1.5)
points(values.of.characteristic,p,pch=19,
       col="red",cex=0.5)
points(values.of.characteristic,rep(0,26),
       pch=19,cex=0.5,col="red")
for(x in c(5,8,12)){
 arrows(x,0, x,p[x+1],col="red",length=0.08,angle=25 )
 arrows(x,p[x+1],0, p[x+1],col="red",
       length=0.08,angle=25)
 text(0,p[x+1], toString(round(p[x+1],2)),
     adj=c(1.05,0.5), cex=0.85 )
 text(x,0, toString(x),
     adj=c(0.5,1.3) )
}

p = seq(0.10,0.90,0.10) 
q = qgamma(p,shape=SHAPE,scale=SCALE)
points(q,p,pch=19,cex=0.5,col="blue")
points(rep(0,9),p,pch=19,cex=0.5,col="blue")

for(y in c(1,5,8)){
arrows(0,p[y], q[y],p[y],
       col="blue",length=0.08,angle=25 )
arrows(q[y],p[y], q[y],0,  
       col="blue",length=0.08,angle=25 )
text(q[y],0, toString(round(q[y],1)),
     adj=c(0.5,1.3) )
text(0,p[y], sprintf("%.2f", p[y]),
     adj=c(1.05,0.5), cex=0.85 )
}

x0=13.5 ; dx=0.45
y0=0.15; dy = 4

segments(x0,y0,x0,y0+0.15*dy)
for(d in seq(0,0.15,0.025)){
  text(x0-dx/3, y0+dy*d, toString(d), cex=0.65,adj=c(1,0.5))
  segments(x0, y0+dy*d, x0-dx/4,y0+dy*d) 
}
values=seq(0,25,0.01)
d = dgamma(values,shape=SHAPE,scale=SCALE)
lines(x0+values*dx,y0+d*dy,col="grey55")
segments(x0,y0,x0+dx*max(values.of.characteristic),y0)
for(x in c(0,5,12,20,25)) text(x0+x*dx,0.95*y0,toString(x),adj=c(0.5,1.1))
x=seq(0,12,0.1)
d = dgamma(x,shape=SHAPE,scale=SCALE)
polygon(x0+dx*c(x,max(x),0),y0+dy*c(d,0,0),col="red",border=NA)
P = pgamma(max(x),shape=SHAPE,scale=SCALE)
text(x0+dx*max(x)/2,y0+dy*max(d)/3,sprintf("%.2f",round(P,2)),
     adj=c(0.5,0.5), cex=1 )


xx = x0+dx*20
yy = y0+dy*0.055

arrows(xx-3*dx,yy,xx-5*dx,yy/2,length=0.07,angle=30)

text(xx,yy,"probability\ndensity\nfunction") 

text(xx,yy/0.8,"(dDistribution)", family="mono")


```

## Shapes of Distributions

There are a lot of misconceptions here. Part of the confusion stems from not **distinguishing**  distributions of **individual values** (the theme in this chapter) from the (conceptual) distributions of **statistics** (summaries such as means, proportions, regression slopes) computed from samples of individual values.

Distributions of the second type are called sampling distributions, and their shapes cannot be observed ... unless we simulate them, or work them out using the mathematical statistics. Otherwise, they are unseen, virtual, conceptual. The sample size (the $n$)  plays a central (pun intended) role  in the shape, even more so than the shape of the distribution being sampled from. Sampling distributions are the topic of the next computing chapter.

For now, in this chapter, we are only concentrate on distributions of individual values. The variation of these individual values can be due just to measurement error, or to genuine biological variation, or -- in the case of the pedictions about the date of the ice break-up  -- differences in individual perrceptions and knowledge. 

The main points concerning the **shape of a distribution of individual values** are that:

* The **size of the population has nothing to do with the shape**. If we could measure the distribution of the _diameters_ of one's red blood cells, the size distribution should look the same whether you are smaller person or a larger person, or male or female. Moreover, the distribution of individuals' red bllod cell _concentrations_ would be the same in  provinces/states/countries with larger/smaller populations of persons. 

* There is **no default  shape.** The shape is determined by the context

* This is especially the case if the values are **determined/generated by humans** and human behaviour/choice and by circumstances. A good example is the distribution of finishing times of marathon runs. In one setting you might find peculiarities such as seen in 
[this dataset](http://www.medicine.mcgill.ca/epidemiology/hanley/statbook/HumanBehaviourDistributionsMarathons.pdf#page=14). In another, it might have [very different shapes in those who are and are not elite runners](https://susanli2016.github.io/Boston-Marathon/) and whether there is a 'qualifying' time requirement -- see 'Elite and the rest of us' in the link. 

* Another example are the distributions of hospital size (no. of beds) in different countries or states. [Likewise with the sizes of schools and universities] These might be determined by government policy. In former communist countries, there are often cookie-cutter hospitals, all the same size within a region, and given the names hospital # 1, hospital # 2, etc. In other countries, they might reflect variations in population density, or historical reasons.

* The year-of-manufacture distribution of cars registered in the the UK has some **'person-made' (and mainly 'man'-made) non-smoothness**. Debate continues as to whether the further 'irregularities' that will be evident by the end of 2020 were man-made or nature-made.

* Yet another examples are age-distributions: **many factors determine the age-structure** of a country or region of a country. It used to be like a pyramid, and still is in less developed countries. See here for some historical and modern 
[Canadian](https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/pyramid/pyramid.cfm?type=1&geo1=01) and [world](https://www.visualcapitalist.com/animation-population-pyramids-10-biggest-countries/) examples.

* The **distribution of age-at-death** is determined by the age-distribution of the living -- and by the many factors that drive age-specific death rates.

* The (sex-specific) distribution of **adult heights** of a homogeneous population might be close to symmetric (and even close to Gaussian), since the heights are determined by a large number of both 'nature' and 'nurture' factors. But, the (sex-specific) distribution of **adult weights** would not. Weight is determined in part by height, but also by a large number of 'elective' factors that are determined by one's own lifestyle, and by the environment one lives in. In over-nourished North America, the distribution has a long right tail, whereas in some under-nourished parts of the world, it would have a long left tail.  

* Why does the distribution of **blood levels of lead** in 
[women of child bearing age](http://www.medicine.mcgill.ca/epidemiology/hanley/c607/ch01/homegrown_exercises_01.pdf#page=2) have a long right tail, and why is it better fiot by a log-'normal' than a 'normal' curve? Note that the shape would not change much if  a bigger sample had been taken. The histogram would just be smoother. Might it have to do with subpopulations of persons with different other sources of exposure besides Glasgow's drinking water? For older distributions of blood lead, see here. For modern ones, use Google. 

* The distribution of the lengths of flight delays might well be determined by competition, regulation, location, etc.

* 'Then God said, 'Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.' Thus God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament; and it was so. And God called the firmament Heaven. So the evening and the morning were the second day' (Genesis 1:6-8). Did 'God' try to make it so that the **distribution of the depths of the ocean**  would have a Gaussian curve?

* The distribution is a property of the population or the owner.

* The testing for normality that is so common needs to stop. There are **a few individual-based contexts** (such as with growth curves, where the percentile where an individual is located on a growth curve critical, or in tests to detect doping in sports, where an athlete's [test value is located in a reference distribution](http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/TestingForHumanIGFdoping.pdf)) where the **shape is critical.** However, when we are interested in **estimating the mean** or another measure of (covariate-patten-specific) distributions, the shape of the distribution of individaul values is not that relevant.   

* See here for [shapes -- some odd -- of distributions of individual values -- and material on graphics](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/DescriptiveStatistics/).

## Exercises

### Guesses in Nenana Ice Classic

Refer again to the guesses about the time of the ice-breakup.

1. Once you have determined the 72 (cumulative) proportions (p's) associated with the 72 half-days, plot them on the vertical axis against the numbers of elapsed days since the beginning of the year on the horizontal axis. Thus the horizontal axis runs from 92 + 10 = 102 days to 92 + 30 + 15 = 137 days.

2. The 1st, 2nd, ... , 98th, 99th percentiles are not so easy to determine since you  have  to locate the 2419th, 4839th, 7258t, ... entries in the 1201-page Book of Guesses and  plot the 99 p's (on the vertical axis) against the 99 (elapsed) times (q's) on the horizontal axis.  Instead, use the first entry on each of pages 11, 21, ... in 
[this excerpt](http://www.biostat.mcgill.ca/hanley/bios691/SampledPages.pdf). [see the shared spreadsheet]. Using a different colour, plot these  slightly-more-dense quantiles on the horizontal axix against the following percentages:

```{r,eval=T, echo=T, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F}

entries = 200*seq(10,1200,10) + 1
percent = 100 * entries/241929
noquote( paste(head(round(percent,1),10),collapse="%, ") )
tail(round(percent,1),10)
```

3. Compare the Q$_{25}$, Q$_{50}$, and Q$_{75}$ obtained directly with the ones obtained by interpolation of the curve showing the results of the other method.

4. Compare the directly-obtained proportions of guesses that are before (the end of) April 20, April 30, and May 10  with the ones obtained by interpolation of the curve showing the results of  the other method.

5. By successive subtractions, calculate the numbers of guesses in each 1/2 day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation.

6. How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage? {see the 2020 brochure}

7. Why did the 'experts' at the 1906 [English country fair](http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf) do so much better that their Alaskan counterparts?

8. Why was the shape of the distribution of guesses by [Dutch casino goers](http://www.medicine.mcgill.ca/epidemiology/hanley/bios691/The_wisdom_of_the_inner_crowd_.pdf) so different from the English and Alaskan ones?

9. Instead of measuring the guessed times from the beginning of the year, suppose that, as [Fonseca et al](http://www.medicine.mcgill.ca/epidemiology/hanley/bios691/VoxPopuliNenana.pdf)
did,  we  measure the guessed times from the spring equinox in Alaska, i.e. from 8:15 a.m. on Tuesday, March 20, 2018, Alaska time. In this scale, compute the mean guess, and the SD of the guesses.

10. Suppose, again, we  measure the guessed times from the spring equinox, but in **weeks**. In this scale, compute the mean guess, and the SD of the guesses.

11. How much warmer/colder in Nov-April is Montreal than Nenana?

12. (For a future assignment, but you can start thinking about how) From a random sample of 100 guesses from the book, estimate how many guesses in the entire book are PM.


```{r,eval=T, echo=T, fig.align="center", fig.height=7, fig.width=9, warning=FALSE, message=F}

my.id = 800606
set.seed(my.id)
n = 50
sample.entry.numbers = sample(x = 1:241929, size=n)
sorted.sample.entry.numbers = sort(sample.entry.numbers)
head(sorted.sample.entry.numbers,10)
page.number = ceiling(sorted.sample.entry.numbers/200)
within.page = sorted.sample.entry.numbers-200*(page.number-1)
column.number = ceiling(within.page/100)
row.number = within.page - 100*(column.number-1)

dataset = data.frame(page.number,column.number,row.number)
head(dataset)
tail(dataset)

```



Some more links on the 'Wisdom of Crowds'

https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/


https://www.all-about-psychology.com/the-wisdom-of-crowds.html


http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf


### Exercise: Ages of UK Cars

Once you have read in the data from the .csv file

1. Sum the row-specific (model-specific) totals, so as to arrive at the total of all registrations. Then look up the population of the UK, and calculate the number of registered cars per capita. Compare this with the corresponding figures for Canada and for the USA. _Hint_: If you have a vector `v`, or a row or a column  in a `data.frame`, then this statement `sum(v,na.rm=TRUE)` will return the overall sum of all of the values in `v`. The  `na.rm = TRUE` option excludes (removes) `NA` values: otherwise, any `NA` values make the sum  `NA` as well.

2. Sum the counts in each column, so as to obtain a vector of year-of-manufacture-specific frequencies. Plot these against the years of manufacture, and comment on the shape of the frequency distribution. _Hint_: If you have a data.frame (or matrix) called `df`, then this statement `apply(df[,4:13], MARGIN=2, FUN=sum, na.rm=TRUE)` will return, as a vector of length 10, the sums of the entries in columns 4 to 13 respectively.  [If you specify MARGIN=1, it applies the `sun`sum` function to each row, and returns row totals]

3. Apply the `cumsum` function in `R` to these frequencies, and plot the cumulative numbers against the year of manufacture. Comment on any any 'remarkable' (^) features. What do you think the graph will look like at thee nd of 2020? 

> (^) In handrwitten medical charting, remarks are often numbered R$_1$, R$_2$, etc. [see here](https://www.quora.com/In-medical-terminology-what-do-remarkable-and-unremarkable-mean). JH's mother was always annoyed when the community nurse would say something was 'unremarkable', but it made sense: in their training, nurses are taught to document (remark on) any deviation from the normal. Just like so many adjectives today, tremarkable has lost some of its specificity.    

4. From this graph, visually estimate the $Q_{25}$, $Q_{50}$ and $Q_{75}$ values. _Hint_: you might use the `abline` function to add 3 horizontal lines to help you. Also, to see what `cumsum` returns, try `cumsum( c(1,2,3,4,5) )` or (for short) `cumsum( 1:5 )`.

5. Use `R` to do formally what you did visually: find the first (earliest) year of manufacture such that the cumulative total exceeeds 25% (50%, 75%) of the total. The `which` function in R 
can help: for example, to find the earliest partial/cumulative sum of the vector `r v = 1:10`  `v =c(``r 1:10``)` to exceed 55/2, you can use `min(which( cumsum(v) > 55/2 ))` to find that it is the
`r min( which(cumsum(v) > 55/2) )`-th one. To see how `it`which` works, use `which( cumsum(v) > 55/2 )` first to see what it returns.


4. It would make the statistics more useful (less particularistic) if you converted the scale from `year of manufacture` to `age`. How would this change of scale affect (a) the shape of the distribution (b) the $Q$'s?


## SUMMARY

### Computing

* Assigning values to objects via `<-` or `=` 

* Putting numbers into vectors  via  concatenation `c( , , )`

* Putting a sequence of values into vectors  via  the `seq()` function

* Looking at the first `n` and the last `n` elements of an object via `head(object,n)` and `tail(object)` -- if you omit the `n`, it defaults to 6 

* Making a new numerical value or vector of numerical values from existing ones via, e.g. via `+` , `*` ( multiplication), `^ power` etc.

* Using `str(object)` to see the **str*ucture of an object

* Using `plot(x,y)` to plot an 'x' vector versus a 'y' vector. `lines(), `points()` and `text()` can be added to an existing plot.

* The `approx()` function for fitting


### Statistical Concepts and Principles 

* Definition of the p (Cumulative Distribution) function of a random variable $Y:$

$$CDF_Y(y) = F_Y(y) =  Prob( Y \le y).$$

* Definition of the q (quantile, or InverseCDF) function of a random variable $Y:$

$$ q_Y(p) : the \ y \ such \ that \ Prob( Y \le q) = p.$$


* The is **no default shape** for the distributions of random variables,  especially those representing **individual values** generated by, or concerning, humans. The shape is determined by the context.



